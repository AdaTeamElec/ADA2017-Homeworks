{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Propensity score matching\n",
    "\n",
    "## 1.1 A naive analysis\n",
    "We start by importing the data and then split them into 2 subgroups: control and treated. We display some informations on the distributions and some plots.\n",
    "\n",
    "*NOTE: we play dumb on purpose for this first \"naive\" analysis. A very succinct analysis of the number and distribution is given, without asking ourselves the typical questions that a data scientist should adress when given any dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display 2 dataframes from the same cell\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv('lalonde.csv')\n",
    "\n",
    "#Create the 2 groups\n",
    "treated = df.loc[df['treat'] == 1]\n",
    "control = df.loc[df['treat'] == 0]\n",
    "\n",
    "# display floats in dataframe with only 2 digits after comma\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "#Informations on both distributions while dropping irrelevant columns\n",
    "display(treated.drop(['id', 'treat', 'black', 'hispan', 'married', 'nodegree'], axis=1).describe())\n",
    "display(control.drop(['id', 'treat', 'black', 'hispan', 'married', 'nodegree'], axis=1).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figures computed, we see that the two groups do not have the same number of participants. We see that the average age of participants is 25.82 for treated group, and 28.03 for control group, which is slighty different but still rather similar. The mean of years studied is almost the same in the two groups. Regarding the incomes, we can see that on average, before treatment (re74 and re 75 entries) the salary of the treated group was lower than the average incomes of the control group. <br>\n",
    "Finally, from the the incomes of 78 (re78), we see that after treatment, the difference of average salary is strongly reduced between the two groups. What is interesting to see is that the range of salary for control group is rather small : from 0 (no incomes-unemployed) to 25'564.67\\$ whereas for the treated group, the range is way bigger and goes from 0 to 60'307.93\\$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "fig, axes = plt.subplots(2,2, figsize=(16,6))\n",
    "sns.distplot(treated['re78'], kde=False, ax=axes[0,0], bins=30)\n",
    "sns.kdeplot(treated['re78'], ax=axes[0,1])\n",
    "sns.distplot(control['re78'], kde=False, ax=axes[1,0], bins=30)\n",
    "sns.kdeplot(control['re78'], ax=axes[1,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a naive analysis we can see that the controled group seems to have a more spread distribution within its range, but that could be related to the higher number of samples. In comparison, the treated group has many subjects whose salary is in the range 0-15'000\\$ and few subject with very high salaries. In both cases, the mean is on the right side of the median and we can therefore conclude that they are both positively skewed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 A closer look at the data\n",
    "\n",
    "### Distributions from the treated group\n",
    "\n",
    "One should notice that we should analyse binary features (id, black, hispan, married, nodegree) differently than non-binary features (age, educ, re74, re75 and re78).\n",
    "#### Non-Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_distributions(table1, table2, exclude):\n",
    "    '''Plot 3 graphs for each column of both dataframes (same number of columns needed): boxplot, distplot and kdeplot.\n",
    "    Possibility to exclude some features. '''\n",
    "    fig, axes = plt.subplots(len(table1.drop(exclude, axis=1).columns), 3, figsize=(16,10))\n",
    "    i = 0\n",
    "    for col in table1.drop(exclude, axis=1):\n",
    "        '''print('Comparaison of the column {}:'.format(col))'''\n",
    "        sns.boxplot(table1[col], orient='h', width=0.4, ax=axes[i, 0])\n",
    "        sns.boxplot(table2[col], orient='h', width=0.4, ax=axes[i, 0])\n",
    "\n",
    "        sns.distplot(table1[col], kde=False, ax=axes[i,1], bins=30).set_title(col)\n",
    "        sns.distplot(table2[col], kde=False, ax=axes[i,1], bins=30)\n",
    "\n",
    "        sns.kdeplot(table1[col], ax=axes[i,2])\n",
    "        sns.kdeplot(table2[col], ax=axes[i,2])\n",
    "        i=i+1\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(treated, df, ['id', 'treat', 'black', 'hispan', 'married', 'nodegree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that........\n",
    "\n",
    "#### Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_proportions(dataframe):\n",
    "    proportions = []\n",
    "    for serie in dataframe:\n",
    "        proportions.append(dataframe[serie].sum()/dataframe[serie].count())\n",
    "    return proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "treated_proportions = binary_proportions(treated.iloc[:,[4,5,6,7]])\n",
    "control_proportions = binary_proportions(control.iloc[:,[4,5,6,7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "ax1.set_ylim(0,1)\n",
    "ax2.set_ylim(0.1)\n",
    "sns.barplot(y=treated_proportions, x=['black', 'hispan', 'married', 'nodegree'], orient='v', ax=ax1).set_title('Treated')\n",
    "sns.barplot(y=control_proportions, x=['black', 'hispan', 'married', 'nodegree'], orient='v', ax=ax2).set_title('Control')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the distribution of the \"binary\" features, we see that the racial distibution between the two groups is very different. In the treated group, there is more than 80% of black people and less than 10% of hispanic people, and we can guess that the rest would be white people. In the control group however, there is only 20% of black people and 15% of hispanic, meaning that we have a majority of white people in the control group. As for the other values, the treated group has approximately 20% of marreid people whereas in the control group there is approximately 50% of married subjects. Lastly, the proportion of subjects without a degree is higher in the treated group, approximately 70% against 60% in the control group. <br>\n",
    "All those diffrences in proportion from the control group to the treated group raise the problem of population bias (systematic difference of population within the control group vs the treated group) between the two cohorts observed. This bias will skew the analysis as the comparison between the two cohort is not fair. For a fair comparison, it would be necessary to have similar amount of proportion with specific attributes between the two populations observed.\n",
    "\n",
    "### 1.3 A propensity score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "X = df.drop(['id','treat','re78'], axis=1)\n",
    "y = df['treat']\n",
    "logistic.fit(X, y)\n",
    "y_pred_proba = logistic.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = y_pred_proba[:, 1]\n",
    "\n",
    "display(df.head())\n",
    "display(df.tail())\n",
    "\n",
    "#Create the 2 groups\n",
    "treated = df.loc[df['treat'] == 1]\n",
    "control = df.loc[df['treat'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Balancing the dataset via matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#take only relevant value of df and add a temporary col to it in order to merge\n",
    "treated = treated.loc[:, ['id', 'score']]\n",
    "treated['temp'] = 1\n",
    "\n",
    "#take only relevant value of df and add a temporary col to it in order to merge\n",
    "control = control.loc[:, ['id', 'score']]\n",
    "control['temp'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "difference = []\n",
    "for score_treated in (treated['score']):\n",
    "    for score_controled in (control['score']):\n",
    "        difference.append(abs(score_treated - score_controled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = treated.merge(control, on='temp')\n",
    "newDF['dist'] = difference\n",
    "newDF.drop(['temp'], axis=1, inplace=True)\n",
    "display(newDF.sort_values(by=['dist'], ascending = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : keep lowest distance, check for repetitions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_DF = pd.DataFrame(['id_x', 'id_y', 'dist'])\n",
    "for row in newDF:\n",
    "    if(newDF[row, 'id_x'] ):\n",
    "        final_DF.append(row)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Applied ML\n",
    "\n",
    "## 2.1 Data loading and features extraction\n",
    "\n",
    "First we will download the dataset as explained and take a look at the dataset. It is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups. For more information go to [this website](http://qwone.com/~jason/20Newsgroups/). The data is composed of multiple fields.\n",
    "\n",
    "* `data`: is the actual text and description of the news.\n",
    "* `target`: is the label (as integers) of the news that are part of the 20 categories present in `target_names` (displayed later).\n",
    "* `filename`: is the location of the file.\n",
    "* `descritpion`: is the title of the dataset, here : \"the 20 newsgroups by date dataset\".\n",
    "* `DESCR` is empty\n",
    "* `target_names`: is the list of target we are trying to match (news categories linked to `target`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(newsgroups.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some labels are actually sub categroies. For example `politics.guns` and `politics.mideast` are both subcategories of `talk`. We can expect it will be more difficult to discriminate and classify news that are part of the same category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our TF-IDF matrix. The matrix will be sparse and huge since we are not removing any words or limiting the number of features. Here for example the size of vocabulary (features) is 173762."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "x_tfidf = tfidf.fit_transform(newsgroups.data) \n",
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As asked we will split our data in 3 different sets: `train` (80%), `validation` (10%) and `test` (10%). We will assert our results on the validation set before testing it on the test set. Note that we fixed the seed to 0 for reproducibility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratio_train = 0.8\n",
    "ratio_validation = 0.1\n",
    "\n",
    "np.random.seed(0)\n",
    "id_ = np.random.permutation(np.arange(x_tfidf.shape[0]))\n",
    "id_train = id_[:np.ceil(x_tfidf.shape[0]*ratio_train).astype(int)]\n",
    "id_validation = id_[np.floor(x_tfidf.shape[0]*ratio_train).astype(int):\n",
    "                    np.ceil(x_tfidf.shape[0]*(ratio_train+ratio_validation)).astype(int)]\n",
    "id_test = id_[np.floor(x_tfidf.shape[0]*(ratio_train+ratio_validation)).astype(int):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we randomly split our data ids we can create our features sets and labels for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_tfidf[id_train]\n",
    "x_validation = x_tfidf[id_validation]\n",
    "x_test = x_tfidf[id_test]\n",
    "\n",
    "y_train = newsgroups.target[id_train]\n",
    "y_validation = newsgroups.target[id_validation]\n",
    "y_test = newsgroups.target[id_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Classification using Random Forest\n",
    "\n",
    "### 2.2.1 Train and Validation set\n",
    "\n",
    "We decided to go from 0 to around 200 features for both `max_depth` and `n_estimators`. Note that we choosed a logaritmic scale for features numbers since there are no resons to compare values such as for example `max_depth`=63 to `max_depth`=64 since they will give similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "span_depth = np.ceil(np.logspace(0, 2.3, 15)).astype(int)\n",
    "span_estimators = np.ceil(np.logspace(0, 2.3, 15)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = np.zeros((span_depth.shape[0], span_estimators.shape[0]))\n",
    "\n",
    "for i, n_depth in enumerate(span_depth):\n",
    "    for j, n_estimator in enumerate(span_estimators):\n",
    "        clf = RandomForestClassifier(max_depth=n_depth, n_estimators=n_estimator, random_state=0, n_jobs=-1)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_validation)\n",
    "        score[i, j] = accuracy_score(y_pred, y_validation)\n",
    "    print('Step {}/{} - Best accuracy max_depth={} is {:.4f}'.format(\n",
    "        i+1, len(span_depth), n_depth, np.max(score[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the grid search takes some time to run we saved the results to save time for next runs. Of course you can run the code from scratch at any time to test the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('score_random_forest.npy', \n",
    "        {'max_depth': span_depth, 'n_estimators': span_estimators, 'score': score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('score_random_forest.npy')[()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the accuracies and we observe that the larger the parameters `n_estimators` and `max_depth` are, the better the accuracy is. However we can also observe that we are reaching some limit. It seems that the algorithm is reaching a plateau around 86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_score = pd.DataFrame(data['score'])\n",
    "df_score.columns = [str(num) for num in data['n_estimators']]\n",
    "df_score.index = [str(num) for num in data['max_depth']]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df_score, annot=True, fmt=\".2f\", cmap='RdYlGn')\n",
    "ax.set_xlabel('n_estimators'); ax.set_ylabel('max_depth');\n",
    "ax.set_title('Accuracy score over \"Max Depth\" and \"N Estimators\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can even more clearly see that we reached a plateau. There is only 4% augmentation in accuray for a difference of 169 in # of Estimators. As a consequence, we choosed to not try with higher values since they will not represent a significative improvement in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_score.index, df_score.iloc[-1], label='max_d=200')\n",
    "plt.plot(df_score.index, df_score.iloc[-3], label='max_d=94')\n",
    "plt.plot(df_score.index, df_score.iloc[-6], label='max_d=31')\n",
    "plt.xlabel('# Estimators'); plt.ylabel('Accuracy in %'); plt.grid(); \n",
    "plt.title('Accuracy on validation set as a funtion of MaxDepth and #Estimators'); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Results on Test set\n",
    "\n",
    "We used the values we best score on validation set e.i. (Max Depth, # Estimators) = (200, 200). We can see that the result on the test set is similar to the one on the validation set with around 86% in accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=200, n_estimators=200, random_state=0)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print('Accuracy on train set is {:.4f}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look more carefuly at the results we can compute the confusion matrix. Note that, by default, the matrix is not normalized and it is difficult to look at coherence of the results. We choosed to normalize it with \n",
    "\n",
    "$$\\text{precision}_j = \\frac{\\text{tp}_j}{\\text{tp}_j + \\sum_{i \\neq j} \\text{fp}_i} $$\n",
    "\n",
    "where $\\text{tp}_j$ is the amount of true positive and $\\text{fp}_i$ are the number of false positive for each class $j$. Note that we did not consider recall in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred, y_test)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_conf = pd.DataFrame(cm)\n",
    "df_conf.columns = newsgroups.target_names\n",
    "df_conf.index = newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most classes are performing well with values close to 1. However we can see that some classes such as `soc.religion.christian` only have around 0.7 precision. It is interesting to notice that most of the fp are acually part of `talk.religion.misc` and fewer are `alt.atheism`. Those two topics are indeed related to the first one and we can conclude classification is harder. It is also interesting to notice that `misc.forsale` have fp spread around other classes such as `sci.electronics`, `comp.sys.mac.hardware`, or even `comp.graphics`. Thoses make also sense since this category (forsale) can include a lot of electronic devices and miscellaneous objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "ax = sns.heatmap(df_conf, annot=True, fmt=\".2f\", cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Feature Importances\n",
    "Let's now look at the features more carefuly. If we plot the repartition of the feature importances we can see that there are actually a lot of features that have little importance in classification (close to 0). Moreover there are only around 20 feature that are above 0.002. Note that the y scale is logaritmic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(clf.feature_importances_, bins=50)\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.xlabel('Feature importance'); plt.ylabel('#Features');\n",
    "plt.grid(); plt.title('Repartition of feature importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained before it seems that a small set of top feature have huge impact on clssification. Let's display the 50 with highest importance and therefore look at the relevant words use for classification. It is interessting to notice that some words are actually part of the categories (labels) names. For example \"baseball\" with `rec.sport.baseball`, \"gun\" with `talk.politics.gun` or \"sale\" with `forsale`. However it is wierd to have some unrelevant words such as \"are\", \"you\", \"for\", \"the\" or even \"it\" that are often considered as stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "id_max = np.argsort(clf.feature_importances_)[-n_features:]\n",
    "np.array(tfidf.get_feature_names())[id_max]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
