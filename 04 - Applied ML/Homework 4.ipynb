{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Propensity score matching\n",
    "\n",
    "## 1.1 A naive analysis\n",
    "We start by importing the data and then split them into 2 subgroups: control and treated. We display some informations on the distributions and some plots.\n",
    "\n",
    "*NOTE: we play dumb on purpose for this first \"naive\" analysis. A very succinct analysis of the number and distribution is given, without asking ourselves the typical questions that a data scientist should adress when given any dataset.*\n",
    "\n",
    "Below we import the datasets and apply the pandas function `describe()` to compute the mean, standard deviation, minimum, quartiles, median and maximum to have a better idea of some features of the dataset. We purposely decided to leave the binary features aside as computing the aformentionned figures about those features is not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display 2 dataframes from the same cell\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv('lalonde.csv')\n",
    "\n",
    "#Create the 2 groups\n",
    "treated = df.loc[df['treat'] == 1]\n",
    "control = df.loc[df['treat'] == 0]\n",
    "\n",
    "# display floats in dataframe with only 2 digits after comma\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "#Informations on both distributions while dropping non-relevant columns to describe\n",
    "display(treated.drop(['id', 'treat', 'black', 'hispan', 'married', 'nodegree'], axis=1).describe())\n",
    "display(control.drop(['id', 'treat', 'black', 'hispan', 'married', 'nodegree'], axis=1).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figures computed, we see that the two groups do not have the same number of participants. We see that the average age of participants is 25.82 for treated group, and 28.03 for control group, which is slighty different but still rather similar. The mean of years studied is almost the same in the two groups. Regarding the incomes, we can see that on average, before treatment (re74 and re 75 entries) the salary of the treated group was lower than the average incomes of the control group. <br>\n",
    "Finally, from the the incomes of 78 (re78), we see that after treatment, the difference of average salary is strongly reduced between the two groups. What is interesting to see is that the range of salary for control group is rather small : from 0 (no incomes-unemployed) to 25'564.67\\$ whereas for the treated group, the range is way bigger and goes from 0 to 60'307.93\\$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the two groups\n",
    "fig, axes = plt.subplots(2,2, figsize=(16,6))\n",
    "plt.suptitle('Distribution of incomes')\n",
    "axes[0,0].set_title('Treated group');\n",
    "sns.distplot(treated['re78'], kde=False, ax=axes[0,0], bins=30)\n",
    "sns.kdeplot(treated['re78'], ax=axes[0,1])\n",
    "axes[1,0].set_title('Control group')\n",
    "sns.distplot(control['re78'], kde=False, ax=axes[1,0], bins=30)\n",
    "sns.kdeplot(control['re78'], ax=axes[1,1])\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a naive analysis we can see that the controled group seems to have a more spread distribution within its range, but that could be related to the higher number of samples. In comparison, the treated group has many subjects whose salary is in the range 0-15'000\\$ and few subject with very high salaries. In both cases, the mean is on the right side of the median and we can therefore conclude that they are both positively skewed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 A closer look at the data\n",
    "\n",
    "### Distributions from the treated group\n",
    "\n",
    "One should notice that we should analyse binary features (black, hispan, married, nodegree) differently than non-binary features (age, educ, re74, re75 and re78). We will begin by looking further into the non binary features.\n",
    "#### Non-Binary Features\n",
    "To have a better understanding of the distribution of each non-binary features, we decided to plot violinplot, which countain similar informations as a boxplot (quartiles, median, min/max value) in addition to a kernel density plot. Note that for each plot, the control group appears on the top whereas the the treated group is on the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions_nbinary(df, compare_col):\n",
    "    '''Plot comparative graphs for selected features of both dataframes'''\n",
    "    plt.figure(figsize=(16,12))\n",
    "    for i, col in enumerate(compare_col):\n",
    "        # Violinplot to compare data\n",
    "        ax = plt.subplot(3, 2, i+1)\n",
    "        sns.violinplot(data=df, x=compare_col[i], y='treat', ax=ax, inner=\"quartile\", palette='Set2', orient='h');\n",
    "        ax.set_title('Feature: ' + compare_col[i])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions_nbinary(df, ['age', 'educ', 're74', 're75', 're78'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe some differences between the distributions. The control group seems to be younger, more spread (probably due to the higher number of samples), the revenues in 74, 75 and 78 are a bit higher, at least in mean than the treated group. Nevertheless, the education feature reveals a similar distribution for both groups.\n",
    "These informations couldn't be seen with the naive analysis above. Now we can focus on the other (binary) features:\n",
    "\n",
    "#### Binary Features\n",
    "\n",
    "<font color=red> Nico pas d'accord ici. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binary_proportions(df, columns, width = 0.3 ):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "    r = df.groupby('treat').sum()[columns]\n",
    "    r = r.div(r.sum(axis=1), axis=0)\n",
    "    \n",
    "    ids_ = np.arange(len(columns))\n",
    "    p1 = plt.bar(ids_, r.iloc[0], width, alpha=0.7, label='control')\n",
    "    p2 = plt.bar(ids_+width, r.iloc[1], width, alpha=0.7, label='treated')\n",
    "    \n",
    "    plt.xlabel('Different classes')\n",
    "    plt.ylabel('Ratio of people')\n",
    "    plt.title('Repartition of subject along different categories')\n",
    "    plt.xticks(ids_ + width / 2, tuple(columns))\n",
    "    plt.ylim(0,1)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_proportions(df, ['black', 'hispan', 'married', 'nodegree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> Observations bas√©es sur l'ancien plot qui n'ont plus de sens ici... </font>\n",
    "\n",
    "By looking at the distribution of the \"binary\" features, we see that the racial distribution between the two groups is very different. In the treated group, there is more than 80% of black people and less than 10% of hispanic people, and we can guess that the rest would be white people. In the control group however, there is only 20% of black people and 15% of hispanic, meaning that we have a majority of white people in the control group. As for the other values, the treated group has approximately 20% of marreid people whereas in the control group there is approximately 50% of married subjects. Lastly, the proportion of subjects without a degree is higher in the treated group, approximately 70% against 60% in the control group. <br>\n",
    "All those differences in proportion from the control group to the treated group raise the problem of population bias (systematic difference of population within the control group vs the treated group) between the two cohorts observed. This bias will skew the analysis as the comparison between the two cohort is not fair. For a fair comparison, it would be necessary to have similar amount of proportion with specific attributes between the two populations observed.\n",
    "\n",
    "### 1.3 A propensity score model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **linear_model** from sklearn in order to define the propensity score for each sample. In order to apply our logistic regression we drop 2 columns for the input: **id** since it doen't contain relevant information and **treat** because it is our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "X = df.drop(['id','treat'], axis=1)\n",
    "y = df['treat']\n",
    "logistic.fit(X, y)\n",
    "y_pred_proba = logistic.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = y_pred_proba[:, 1]\n",
    "display(df[182:188])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then have a final dataframe with the propensity score for each row.\n",
    "\n",
    "### 1.4 Balancing the dataset via matching\n",
    "\n",
    "We will balance our dataset using the [**linear_sum_assignment**](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.linear_sum_assignment.html) from Scipy. This function will allow us to compute the minimum weight matching between the scores from the treated and control group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "#Create the 2 groups\n",
    "treated = df.loc[df['treat'] == 1]\n",
    "control = df.loc[df['treat'] == 0]\n",
    "\n",
    "# Create a matrix with distances between every subjects\n",
    "dist = np.zeros([len(treated['score']), len(control['score'])])\n",
    "for i, t in enumerate(treated['score']):\n",
    "    for j, c in enumerate(control['score']):\n",
    "        dist[i, j] = abs(t-c)\n",
    "\n",
    "# We use scipy to check that the sum of absolute propensity-score differences \n",
    "# between the two matched subjects is minimized.\n",
    "row_ind, col_ind = linear_sum_assignment(dist)\n",
    "matched_df = pd.concat([treated, control.iloc[col_ind]])\n",
    "\n",
    "# The first 185 subjects are from treated groupe and match to the last \n",
    "# 185 subjects such as 0 is match with 185, 1 with 186 and so on, n with n+185...\n",
    "matched_df.iloc[[1, 1+185]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check that we have unique matching values (2x185). \n",
    "matched_df['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "fig, axes = plt.subplots(2,2, figsize=(16,6))\n",
    "plt.suptitle('Distribution of incomes after matching')\n",
    "axes[0,0].set_title('Treated group');\n",
    "sns.distplot(matched_df.iloc[0:185]['re78'], kde=False, ax=axes[0,0], bins=30)\n",
    "sns.kdeplot(matched_df.iloc[0:185]['re78'], ax=axes[0,1])\n",
    "axes[1,0].set_title('Control group')\n",
    "sns.distplot(matched_df.iloc[185:370]['re78'], kde=False, ax=axes[1,0], bins=30)\n",
    "sns.kdeplot(matched_df.iloc[185:370]['re78'], ax=axes[1,1])\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Informations on both distributions while dropping irrelevant columns\n",
    "display(matched_df.iloc[0:185].drop(['id', 'treat', 'black', 'hispan', 'married', 'nodegree'], axis=1).describe())\n",
    "display(matched_df.iloc[185:370].drop(['id', 'treat', 'black', 'hispan', 'married', 'nodegree'], axis=1).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a better dataset, we can see that revenues re78 of the treated group are a bit higher that the control group. Indeed the mean is around 500$ more than the control group and all of the quantiles are also higher.\n",
    "The treatement may have an effect on the incomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions_nbinary(matched_df, ['age', 'educ', 're74', 're75', 're78', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_proportions(matched_df, ['black', 'hispan', 'married', 'nodegree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a more coherent subset between both groups. Overall profiles are more close to each other. If we look at the shapes of the \"violons\" we will see similar shapes. Nevertheless it's still not perfect. For example, even if all fields have similar distribution in both sets, some differences stll exist e.i. black people ~32% in control set and ~45% in treated set. Same analysis for age distribution. Treated people are overall older compare to control group.\n",
    "\n",
    "Even if this propensity score matching is still not the best solution, we can already assume that the programm have a positive effect on the group and its earnings. \n",
    "\n",
    "\n",
    "### 1.5 Balancing the groups further\n",
    "\n",
    "No, we aren't fully satisfied since there are still differences in distributions, mainly between the races.  \n",
    "One way to solve this issue is to ensure that we are comparing similar subjects. For example it will not make sense to compare an hispanic subject with a black one since there are disparities between races at the time of the survey. Therefore we can check that specific fields are correctly matched, e.i. 'age', 'educ', 'black', 'hispan', 'married', 'nodegree'. We will represent the matching simply as a relative error with \n",
    "\n",
    "$$\\frac{f_{treated}-f_{control}}{f_{control}}< t \\text{, for each feature } f$$\n",
    "\n",
    "Since age and education are not binary values, we will set a threshold $t$ for the confidence value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_selected = ['age', 'educ', 'black', 'hispan', 'married', 'nodegree'] # Features used to compare people\n",
    "\n",
    "df_treated = matched_df.iloc[:185].reset_index()[feat_selected]\n",
    "df_control = matched_df.iloc[185:].reset_index()[feat_selected]\n",
    "\n",
    "res = (df_treated-df_control)/(df_control + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_match_new = np.nonzero(np.sum(np.abs(res) > 0.5, axis=1) == 0)[0]\n",
    "ids_match_new = np.array([ids_match_new, ids_match_new+185]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_new_df = matched_df.iloc[ids_match_new]\n",
    "plot_distributions_nbinary(matched_new_df, ['age', 'educ', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_proportions(matched_new_df, ['black', 'hispan', 'married', 'nodegree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of our newly subset: 76 (38 treated and controled)\n",
    "n_new_match = matched_new_df.shape[0]\n",
    "n_new_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our newly balanced dataset is way smaller than the previous one. We have 38 matches instead of the previous 185. This is a trade-off between the matching accuracy and the sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 A less naive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions_nbinary(matched_new_df, ['re74', 're78'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "fig, axes = plt.subplots(2,2, figsize=(16,6))\n",
    "plt.suptitle('Distribution of incomes after matching 2.0')\n",
    "axes[0,0].set_title('Treated group');\n",
    "sns.distplot(matched_new_df.iloc[0:n_new_match//2]['re78'], kde=False, ax=axes[0,0], bins=30)\n",
    "sns.kdeplot(matched_new_df.iloc[0:n_new_match//2]['re78'], ax=axes[0,1])\n",
    "axes[1,0].set_title('Control group')\n",
    "sns.distplot(matched_new_df.iloc[n_new_match//2:]['re78'], kde=False, ax=axes[1,0], bins=30)\n",
    "sns.kdeplot(matched_new_df.iloc[n_new_match//2:]['re78'], ax=axes[1,1])\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(matched_new_df[:n_new_match//2].describe())\n",
    "display(matched_new_df[n_new_match//2:].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, now that we have a new dataset (smaller but with a better matching), we can see that the programm is definitely worth it. Indeed we can observe that the incomes in re78 are higher for the treated group than for the controled one. In average ~6800\\$ compare to ~4900\\$ which represent a gap of ~1900\\$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Applied ML\n",
    "\n",
    "## 2.1 Data loading and features extraction\n",
    "\n",
    "First we will download the dataset as explained and take a look at the dataset. It is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups. For more information go to [this website](http://qwone.com/~jason/20Newsgroups/). The data is composed of multiple fields.\n",
    "\n",
    "* `data`: is the actual text and description of the news.\n",
    "* `target`: is the label (as integers) of the news that are part of the 20 categories present in `target_names` (displayed later).\n",
    "* `filename`: is the location of the file.\n",
    "* `descritpion`: is the title of the dataset, here : \"the 20 newsgroups by date dataset\".\n",
    "* `DESCR` is empty\n",
    "* `target_names`: is the list of target we are trying to match (news categories linked to `target`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(newsgroups.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some labels are actually sub categories. For example `politics.guns` and `politics.mideast` are both subcategories of `talk`. We can expect it will be more difficult to discriminate and classify news that are part of the same category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our TF-IDF matrix. The matrix will be sparse and huge since we are not removing any words or limiting the number of features. Here for example the size of vocabulary (features) is 173762."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "x_tfidf = tfidf.fit_transform(newsgroups.data) \n",
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As asked we will split our data in 3 different sets: `train` (80%), `validation` (10%) and `test` (10%). We will assert our results on the validation set before testing it on the test set. Note that we fixed the seed to 0 for reproducibility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratio_train = 0.8\n",
    "ratio_validation = 0.1\n",
    "\n",
    "np.random.seed(0)\n",
    "id_ = np.random.permutation(np.arange(x_tfidf.shape[0]))\n",
    "id_train = id_[:np.ceil(x_tfidf.shape[0]*ratio_train).astype(int)]\n",
    "id_validation = id_[np.floor(x_tfidf.shape[0]*ratio_train).astype(int):\n",
    "                    np.ceil(x_tfidf.shape[0]*(ratio_train+ratio_validation)).astype(int)]\n",
    "id_test = id_[np.floor(x_tfidf.shape[0]*(ratio_train+ratio_validation)).astype(int):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we randomly split our data ids we can create our features sets and labels for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_tfidf[id_train]\n",
    "x_validation = x_tfidf[id_validation]\n",
    "x_test = x_tfidf[id_test]\n",
    "\n",
    "y_train = newsgroups.target[id_train]\n",
    "y_validation = newsgroups.target[id_validation]\n",
    "y_test = newsgroups.target[id_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Classification using Random Forest\n",
    "\n",
    "### 2.2.1 Train and Validation set\n",
    "\n",
    "We decided to go from 0 to around 200 features for both `max_depth` and `n_estimators`. Note that we choosed a logaritmic scale for features numbers since there are no resons to compare values such as for example `max_depth`=63 to `max_depth`=64 since they will give similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "span_depth = np.ceil(np.logspace(0, 2.3, 15)).astype(int)\n",
    "span_estimators = np.ceil(np.logspace(0, 2.3, 15)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.zeros((span_depth.shape[0], span_estimators.shape[0]))\n",
    "\n",
    "for i, n_depth in enumerate(span_depth):\n",
    "    for j, n_estimator in enumerate(span_estimators):\n",
    "        clf = RandomForestClassifier(max_depth=n_depth, n_estimators=n_estimator, random_state=0, n_jobs=-1)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_validation)\n",
    "        score[i, j] = accuracy_score(y_pred, y_validation)\n",
    "    print('Step {}/{} - Best accuracy max_depth={} is {:.4f}'.format(\n",
    "        i+1, len(span_depth), n_depth, np.max(score[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the grid search takes some time to run we saved the results to save time for next runs. Of course you can run the code from scratch at any time to test the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('score_random_forest.npy', \n",
    "        {'max_depth': span_depth, 'n_estimators': span_estimators, 'score': score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('score_random_forest.npy')[()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the accuracies and we observe that the larger the parameters `n_estimators` and `max_depth` are, the better the accuracy is. However we can also observe that we are reaching some limit. It seems that the algorithm is reaching a plateau around 86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = pd.DataFrame(data['score'])\n",
    "df_score.columns = [str(num) for num in data['n_estimators']]\n",
    "df_score.index = [str(num) for num in data['max_depth']]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df_score, annot=True, fmt=\".2f\", cmap='RdYlGn')\n",
    "ax.set_xlabel('n_estimators'); ax.set_ylabel('max_depth');\n",
    "ax.set_title('Accuracy score over \"Max Depth\" and \"N Estimators\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can even more clearly see that we reached a plateau. There is only 4% augmentation in accuray for a difference of 169 in # of Estimators. As a consequence, we choosed to not try with higher values since they will not represent a significative improvement in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_score.index, df_score.iloc[-1], label='max_d=200')\n",
    "plt.plot(df_score.index, df_score.iloc[-3], label='max_d=94')\n",
    "plt.plot(df_score.index, df_score.iloc[-6], label='max_d=31')\n",
    "plt.xlabel('# Estimators'); plt.ylabel('Accuracy in %'); plt.grid(); \n",
    "plt.title('Accuracy on validation set as a funtion of MaxDepth and #Estimators'); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Results on Test set\n",
    "\n",
    "We used the values we best score on validation set e.i. (Max Depth, # Estimators) = (200, 200). We can see that the result on the test set is similar to the one on the validation set with around 86% in accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=200, n_estimators=200, random_state=0)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print('Accuracy on train set is {:.4f}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look more carefuly at the results we can compute the confusion matrix. Note that, by default, the matrix is not normalized and it is difficult to look at coherence of the results. We choosed to normalize it with \n",
    "\n",
    "$$\\text{precision}_j = \\frac{\\text{tp}_j}{\\text{tp}_j + \\sum_{i \\neq j} \\text{fp}_i} $$\n",
    "\n",
    "where $\\text{tp}_j$ is the amount of true positive and $\\text{fp}_i$ are the number of false positive for each class $j$. Note that we did not consider recall in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred, y_test)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_conf = pd.DataFrame(cm)\n",
    "df_conf.columns = newsgroups.target_names\n",
    "df_conf.index = newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most classes are performing well with values close to 1. However we can see that some classes such as `soc.religion.christian` only have around 0.7 precision. It is interesting to notice that most of the fp are acually part of `talk.religion.misc` and fewer are `alt.atheism`. Those two topics are indeed related to the first one and we can conclude classification is harder. It is also interesting to notice that `misc.forsale` have fp spread around other classes such as `sci.electronics`, `comp.sys.mac.hardware`, or even `comp.graphics`. Thoses make also sense since this category (forsale) can include a lot of electronic devices and miscellaneous objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "ax = sns.heatmap(df_conf, annot=True, fmt=\".2f\", cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Feature Importances\n",
    "Let's now look at the features more carefuly. If we plot the repartition of the feature importances we can see that there are actually a lot of features that have little importance in classification (close to 0). Moreover there are only around 20 feature that are above 0.002. Note that the y scale is logaritmic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(clf.feature_importances_, bins=50)\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.xlabel('Feature importance'); plt.ylabel('#Features');\n",
    "plt.grid(); plt.title('Repartition of feature importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained before it seems that a small set of top feature have huge impact on clssification. Let's display the 50 with highest importance and therefore look at the relevant words use for classification. It is interessting to notice that some words are actually part of the categories (labels) names. For example \"baseball\" with `rec.sport.baseball`, \"gun\" with `talk.politics.gun` or \"sale\" with `forsale`. However it is wierd to have some unrelevant words such as \"are\", \"you\", \"for\", \"the\" or even \"it\" that are often considered as stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "id_max = np.argsort(clf.feature_importances_)[-n_features:]\n",
    "np.array(tfidf.get_feature_names())[id_max]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
