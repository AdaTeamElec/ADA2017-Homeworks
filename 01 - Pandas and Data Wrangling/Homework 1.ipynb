{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "sns.set_palette('hls')\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "DATA_FOLDER = join('..', '..', 'ADA2017-Tutorials', '02 - Intro to Pandas', 'Data')\n",
    "DATA_EBOLA = join(DATA_FOLDER, 'ebola')\n",
    "DATA_TITANIC = DATA_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average* per year of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Look at the data \n",
    "\n",
    "The reporting method is different in each country. Therefore we have to build different parsers. In our case, we are interested in the **daily average** per year of new **cases** and **death**. The dataset contains a lot of feilds that are useless for us and that we will drop.\n",
    "Note that:\n",
    "\n",
    "* We assume that death in health workers are part of total new death/cases (e.i. total death number = patient death + HW death).\n",
    "* We assurme that if a value is missing, it means that there is no change recorded\n",
    "\n",
    "### 1.2 Guinea\n",
    "Let's first read all csv files and concatenate the datas. We can directly parse the `Date` as a date entry. We fill the missing values (NaN) with 0 since it probably means that no changes were reported. The column `Total` contain the total values for each `Description` (sum of all cities). After parsing we must take a look at the duplicates to see if we have multiple entries for the same tuple (`Date`, `Description`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files and concatenate them\n",
    "guinea_path = join(DATA_EBOLA, 'guinea_data')\n",
    "guinea_files = [join(guinea_path, f) for f in listdir(guinea_path) if isfile(join(guinea_path, f))]\n",
    "\n",
    "r=[]\n",
    "for i in range(len(guinea_files)):\n",
    "    r.append(pd.read_csv(guinea_files[i], usecols=['Description', 'Totals', 'Date'], \n",
    "                         parse_dates=['Date']).fillna(0))\n",
    "    \n",
    "r = pd.concat(r)\n",
    "print('Contains duplicates:', any(r.duplicated(subset=['Date', 'Description'])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no duplicates were found we can pivot the table and keep the `Date` as index and `Totals` in `Description` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = r.pivot_table(index='Date', columns='Description', values='Totals', aggfunc='max').fillna(0)\n",
    "r.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said above, most of the columns are useless in our case. Let's display the entries to choose the columns that contains meaningfull values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fields containing the overall (total) cases/death will be dropes. We will only keep `New cases of confirmed`, `New cases of probables`, `New cases of suspects`, `New deaths registered`, `New deaths registered today (confirmed)`, `New deaths registered today (probables)` and `New deaths registered today (suspects)` since they are more likely to contain meaningfull information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[['New cases of confirmed', 'New cases of probables', 'New cases of suspects', 'New deaths registered', \n",
    "   'New deaths registered today (confirmed)', 'New deaths registered today (probables)', \n",
    "   'New deaths registered today (suspects)']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the fields were not properly parsed (type is object instead of int). Therefore we will apply **to_numeric** funtion to cast them to numbers allowing us to use basic mathematical operation.\n",
    "\n",
    "We create new fields that will be used to merge all the data (for all the countries). `n_case` contains the new cases, `n_case_un` the probable/suspected cases, `n_death` the new registred deaths and `n_death_un` the deaths suspected/probable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r['n_case'] = pd.to_numeric(r['New cases of confirmed'])\n",
    "r['n_case_un'] = pd.to_numeric(r['New cases of probables']) + pd.to_numeric(r['New cases of suspects'])\n",
    "r['n_death'] = pd.to_numeric(r['New deaths registered']) + pd.to_numeric(r['New deaths registered today (confirmed)'])\n",
    "r['n_death_un'] = pd.to_numeric(r['New deaths registered today (probables)']) + pd.to_numeric(r['New deaths registered today (suspects)'])\n",
    "r['country'] = ['guinea']*len(r['n_case'])\n",
    "guinea_res = r[['country', 'n_case', 'n_case_un', 'n_death', 'n_death_un']]\n",
    "guinea_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Liberia\n",
    "Almost the same as Guinea data. We directly parse the `Date` as a date entry. We fill the missing values with 0 since it probably means that no changes were reported. The column `National` contain the total values for each `Variable` (sum of all cities). After parsing we take a look at the duplicates to see if we have multiple entries for the same tuple (`Date`, `Variable`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files and concatenate them\n",
    "liberia_path = join(DATA_EBOLA, 'liberia_data')\n",
    "liberia_files = [join(liberia_path, f) for f in listdir(liberia_path) if isfile(join(liberia_path, f))]\n",
    "\n",
    "r_l=[]\n",
    "for i in range(len(liberia_files)): \n",
    "    r_l.append(pd.read_csv(liberia_files[i], usecols=['Date', 'Variable', 'National'], \n",
    "                         parse_dates=['Date']).fillna(0))\n",
    "    \n",
    "r_l = pd.concat(r_l)\n",
    "print('Contains duplicates:', any(r_l.duplicated(subset=['Date', 'Variable'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contain duplicates. We need to handle them. Let's take a look at the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l[r_l.duplicated(subset=['Date', 'Variable'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that those fields are not relevant. Therefore we can either drop them or merge them. We chose to merge them, using max function, to avoid losing data and to pivot the table as we did for the Guinea data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l = r_l.pivot_table(index='Date', columns='Variable', values='National', aggfunc=max).fillna(0)\n",
    "r_l.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the column are useless in our case. Let's display the entries to choose the columns that contains meaningfull values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fields containing the overall (total) cases/death will be dropes. We will only keep `'New Case/s (Probable)`, `New Case/s (Suspected)`, `New case/s (confirmed)` and `Newly reported deaths)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l[['New Case/s (Probable)', 'New Case/s (Suspected)', 'New case/s (confirmed)', 'Newly reported deaths']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the new fields to match the data schema that we choosed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l['n_case'] = r_l['New case/s (confirmed)']\n",
    "r_l['n_case_un'] = r_l['New Case/s (Suspected)'] + r_l['New Case/s (Probable)']\n",
    "r_l['n_death'] = r_l['Newly reported deaths']\n",
    "r_l['country'] = ['liberia']*len(r_l['n_case'])\n",
    "liberia_res = r_l[['country', 'n_case', 'n_case_un', 'n_death']]\n",
    "liberia_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Sierra Leon\n",
    "Same logic as before. We directly parse the `date` as a date entry. We fill the missing values with 0 since it probably means that no changes were reported. The column `National` contain the total values for each description (sum of all cities). After parsing we take a look at the duplicates to see if we have multiple entries for the same tuple (`date`, `variable`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files and concatenate them\n",
    "sl_path = join(DATA_EBOLA, 'sl_data')\n",
    "sl_files = [join(sl_path, f) for f in listdir(sl_path) if isfile(join(sl_path, f))]\n",
    "\n",
    "r_sl=[]\n",
    "for i in range(len(sl_files)): \n",
    "    r_sl.append(pd.read_csv(sl_files[i], usecols=['date', 'variable', 'National'], \n",
    "                         parse_dates=['date']).fillna(0))\n",
    "    \n",
    "r_sl = pd.concat(r_sl)\n",
    "print('Contains duplicates:', any(r_sl.duplicated(subset=['date', 'variable'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contain duplicates, so let's look at the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl[r_sl.duplicated(subset=['date', 'variable'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those fields are not relevant. Therefore we can either drop them or merge them as we already did for the previous data. We also choose to merge them using max function to avoid data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl = r_sl.pivot_table(index='date', columns='variable', values='National', aggfunc=max).fillna(0)\n",
    "r_sl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the column are useless in our case. Let's display the entries to choose the columns that contains meaningfull values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fields containing the overall (cumulative) of cases/death will be dropes. We will only keep `'new_confirmed`, `new_probable`, `new_suspected` and `death_confirmed` since they are more likely to contain the information we want.\n",
    "\n",
    "Note that `death_confirmed` migth contain the overall value of death. Let's take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl[['new_confirmed', 'new_probable', 'new_suspected', 'death_confirmed']].head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, `death_confirmed` contains the overall number of death. Moreover it also contains 0 since data were probably missing. Let's take also a look at those data with missing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl.loc[r_sl['death_confirmed']==0, ['new_confirmed', 'new_probable', 'new_suspected', 'death_confirmed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all fields are empty we can drop thoses entries. Afterward we can estimate the number of new death as the difference of total registred deaths between two days. Note that for day 1 (first entry in the table) we will not be able to estimate the amount of new deaths. Therefore we chose to put it to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl.drop(r_sl.loc[r_sl['death_confirmed']==0].index, inplace=True)\n",
    "r_sl['new_death'] = pd.to_numeric(r_sl['death_confirmed']).diff().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the new fields to match the data schema that we choosed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl['n_case'] = pd.to_numeric(r_sl['new_confirmed'])\n",
    "r_sl['n_case_un'] = pd.to_numeric(r_sl['new_probable'] + r_sl['new_suspected'])\n",
    "r_sl['n_death'] = pd.to_numeric(r_sl['new_death'])\n",
    "r_sl['country'] = ['sl']*len(r_sl['n_case'])\n",
    "sl_res = r_sl[['country', 'n_case', 'n_case_un', 'n_death']]\n",
    "sl_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all data have the same structure we can concatenate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.concat([guinea_res, liberia_res, sl_res]).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the evolution of number of cases and death for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax2) = plt.subplots(1, 2, figsize=(16,6))\n",
    "for label, df in r.groupby('country'):\n",
    "    df.plot(y='n_death', ax=ax, label=label)\n",
    "for label, df in r.groupby('country'):\n",
    "    df.plot(y='n_case', ax=ax2, label=label)\n",
    "ax.grid(); ax2.grid()\n",
    "ax.set_xlabel('Date'); ax2.set_xlabel('Date')\n",
    "ax.set_ylabel('# Daily deaths'); ax2.set_ylabel('# Daily cases')\n",
    "ax.set_title('Countries - Daily registred deaths'); ax2.set_title('Countries - Daily registred cases')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1. Negative number of death\n",
    "Note that for Sierra Leon (sl), there is a number of registred death negative around early october. This is, of course, not due to the fact that people resuscitated. Here is a more detailed view of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl.iloc[40:50][['death_confirmed', 'new_death']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.loc[r['n_death'] < 0, 'n_death'] = 0\n",
    "###### TODO ############# fix 0 and 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem appears between the 2014-09-30 and 2014-10-01 where the number of death decreased. The problem seems to be a typo (550 typed instead of 530) or a wrong estimation of number of death in a specific city. In both case we are not able to determine the real value of this field so we put 2014-09-30 to **0** (no changes) and 2014-10-01 to **5** (532-527) to be consistant with data.\n",
    "\n",
    "#### 1.5.2. Liberia sudden cases peak\n",
    "We can also see a huge peek at the end for the Liberia. it could be a sudden increase of registred cases but there is no correlation with the number of death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l.loc['2014-12-01':'2014-12-09'][['Total probable cases', 'New Case/s (Probable)',  \n",
    "                                    'Total suspected cases', 'New Case/s (Suspected)', \n",
    "                                    'Total confirmed cases', 'New case/s (confirmed)', 'Newly reported deaths']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.drop(r.loc[np.logical_and(r.index >= '2014-12-04', r['country']=='liberia')].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.3. Final plot and results (cleared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax2) = plt.subplots(1, 2, figsize=(16,6))\n",
    "for label, df in r.groupby('country'):\n",
    "    df.plot(y='n_death', ax=ax, label=label)\n",
    "for label, df in r.groupby('country'):\n",
    "    df.plot(y='n_case', ax=ax2, label=label)\n",
    "ax.grid(); ax2.grid()\n",
    "ax.set_xlabel('Date'); ax2.set_xlabel('Date')\n",
    "ax.set_ylabel('# Daily deaths'); ax2.set_ylabel('# Daily cases')\n",
    "ax.set_title('Countries - Daily registred deaths'); ax2.set_title('Countries - Daily registred cases')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can finally compute the number of death and new cases. We add in the table the `n_case_tot` and `n_death_tot` to take into account the probable cases/deaths. Each column is expressed as the **daily average** case/death **per year**\n",
    "\n",
    "We can see on the next graph that the number of death per day (`n_death`) is more important in Liberia and Sierra Leon . If we consider the probable cases `n_case_tot`, the are more cases of ebola in Sierra Leon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r['n_case_tot'] = r['n_case'] +  r['n_case_un']\n",
    "r['n_death_tot'] = r['n_death'] +  r['n_death_un']\n",
    "\n",
    "COUNTRIES = ['guinea', 'liberia', 'sl']\n",
    "ds = [(r[r['country']==COUNTRY].index[-1]-r[r['country']==COUNTRY].index[0]).days for COUNTRY in COUNTRIES]\n",
    "print('Days spans: {}'.format(ds))\n",
    "r.groupby('country').sum().divide(ds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microbiome_path = join(DATA_FOLDER, 'microbiome')\n",
    "\n",
    "# Read 9 first speadsheets and concatenate them\n",
    "microbiome_files = [join(microbiome_path, f) for f in listdir(microbiome_path) if isfile(join(microbiome_path, f))]\n",
    "microbiome_files.sort()\n",
    "\n",
    "#Save and remove the metadata path\n",
    "metadata = microbiome_files[-1]\n",
    "microbiome_files.pop();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[]\n",
    "for i in range(len(microbiome_files)):\n",
    "    df_temp = pd.read_excel(microbiome_files[i], header=None, names=['Name', 'Number'])\n",
    "    df_temp['src'] = i\n",
    "    arr.append(df_temp)\n",
    "\n",
    "arr = pd.concat(arr)\n",
    "arr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_header = pd.read_excel(metadata)\n",
    "col_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of duplicates: {}'.format(np.sum(arr.duplicated(subset=['Name', 'src']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_merged = arr.pivot(index='Name', columns='src', values='Number')\n",
    "arr_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = col_header[['GROUP', 'SAMPLE']].fillna('unknown').values.T\n",
    "index_new = pd.MultiIndex.from_tuples(list(zip(*t)), names=['GROUP', 'SAMPLE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_merged.columns = index_new\n",
    "arr_merged.fillna('unknown', inplace=True)\n",
    "arr_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Is index unique: {}'.format(arr_merged.index.is_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "# HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "df = pd.read_excel(join(DATA_TITANIC, 'titanic.xls'),  \n",
    "                   converters={'pclass': np.int, 'survived': np.int, 'age': np.float})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about how data are organized cloumn by column \n",
    "* `pclass`: As describe in titanic.html, this field can take 1,2,3 as value. It can be categorical. \n",
    "* `survived`: Binary class, no particular need to be categorical. Actually more usefull to kep it as numerical to plot survival ratio.\n",
    "* `name`: Passenger name. Simple string value. No categorical\n",
    "* `sex`: Sex of the personne. Can be categorical (female, male)\n",
    "* `age`: Age of the passenger, should be in range 0-100 (normally) and be an number. No categorical\n",
    "* `slbsp`/`parch`: Familly related field. Should be an positive integer. No categorical\n",
    "* `ticket`: Id of the ticket. No categorical\n",
    "* `fare`: Fare, expressed in British pound. Range 0 to ... a lot! No categorical\n",
    "* `cabin`: Contains the number and the floor.\n",
    "    * `floor`: Letter indicate the floor number. Categorical\n",
    "* `embarked`: Embarcation location. Categorical Cherbourg (C), Queenstown (Q), Southampton (S)\n",
    "* `boat`: boat id number\n",
    "* `body`: Body id number\n",
    "* `home.dest`: Home and final destination of the passenger\n",
    "\n",
    "Note that we do not have to control each column. We need check the data for `pclass`, `survived`, `sex`, `age`, `cabin`, `embarked`. If ranges was no specified it means that the values can either be a string (no limit) either be a mix between numbers and strings (`cabin` for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Clean data\n",
    "\n",
    "* `pclass`: First we check it contains indeed only 3 values (1,2,3). The we set it as categorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique categories: {}'.format(df['pclass'].unique()))\n",
    "df['pclass'] = df.pclass.astype('category', ordered=True)\n",
    "df['pclass'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `survived`: First we check it contains indeed only 2 values (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique categories: {}'.format(df['survived'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `sex`: It should contains only 2 values (female, male). Then we set it as categorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique categories: {}'.format(df['sex'].unique()))\n",
    "df['sex'] = df.sex.astype('category', ordered=True)\n",
    "df['sex'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `age`: It should be a positive number and not too high or NaN. We have 263 personne with no registered age and an age value in range [0.1667, 80]. There are a lot of passengers that do not have age entries. We will let them set as NaN and discard them for plotting.\n",
    "\n",
    "Note that we were sceptical when we saw that the minimum age was 0.1667 since it not an integer. But as you can see (in the next paragraph) all the values between 0 and 1 are actually the age of the baby in month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Amount of NaN: {}, max: {}, min: {}'.format(\n",
    "    np.sum(pd.isnull(df['age'])), df['age'].max(), df['age'].min()))\n",
    "print('Baby age in range 0-1 : {} month/s'.format( \n",
    "    np.round(pd.to_numeric(df[df['age'] < 1]['age'].values*12), decimals=1) ))\n",
    "df.age = pd.to_numeric(df.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `cabin`: Here we can split the field and get the floor letter. To do so we use regex. Since some fields contains multiple letters we only keep the first one as the are the same. ex: `B58 B60` -> `BB` -> `B`. Note that `n` is not a floor but the abr√©viation of `nan`, so thoses are missing values. We set them to NaN and we don't drop them since there is only 2 values. We will only not take them into account when plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floors = [re.sub(r'[0-9 ]', '', str(item))[0] for item in df['cabin']]\n",
    "print('Floors (unique): {}'.format(np.unique(floors)))\n",
    "df['floor'] = floors\n",
    "df.loc[df['floor'] == 'n', 'floor'] = np.NaN\n",
    "df['floor'] = df.floor.astype('category', ordered=True)\n",
    "df['floor'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `embarked`: It should only contains three values Cherbourg (C), Queenstown (Q), Southampton (S). However we can see that it contains also unknown values (nan). As you see there are only 2 values that containt NaN. THerefore we will let them set to NaN and not include them in `embarked` plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique categories: {}, Unknown: {}'.format(df['embarked'].unique(), df['embarked'].isnull().sum()))\n",
    "df['embarked'] = df['embarked']\n",
    "df['embarked'] = df.embarked.astype('category', ordered=True)\n",
    "df['embarked'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Histogram data\n",
    "We plot the repartition of the passenger as a function of `pclass`, `sex`, `embarked` and `age`. Note that, as said before, NaN values are droped temporary and are not part of the results since they do not represent usefull data.\n",
    "\n",
    "Note that we split the `age` data into subsets (age ranges) as asked for histogram plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_bar_plot(data, ax, title='', y_axis=''):\n",
    "    ax.set_title(title , fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(data.name); ax.set_ylabel(y_axis)\n",
    "    sns.barplot(x=data.value_counts().keys(), y=data.value_counts().values,  ax=ax)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.setp(labels, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16,12))\n",
    "\n",
    "nice_bar_plot(df['pclass'], axes[0, 0], 'Repartition class', '#persons')\n",
    "nice_bar_plot(df['sex'], axes[0, 1], 'Repartition sex', '#persons')\n",
    "nice_bar_plot(df['embarked'], axes[1, 0], 'Repartition embarc. location', '#persons')\n",
    "age_cut = pd.cut(df.age, [0, 10, 20, 30, 40, 50, 60, 70, df.age.max()])\n",
    "nice_bar_plot(age_cut, axes[1, 1], 'Repartition age', '#persons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Piecharts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('hls', 10)\n",
    "def nice_pie_plot(data, ax, title=''):\n",
    "    ax.set_title(title , fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(data.name)\n",
    "    ax.pie(data.value_counts().values, labels=data.value_counts().keys(), autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "nice_pie_plot(df['floor'], ax, 'Repartition of passenger according to floor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('hls', 2)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16,5))\n",
    "for i, (label, df_sub) in enumerate(df.groupby('pclass')):\n",
    "    df_sub['survived'].value_counts(sort=False).plot.pie(\n",
    "        legend=True, ax=axes[i], title='Survived ratio Class{}'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.factorplot(x=\"pclass\", y=\"survived\", hue=\"sex\", data=df, size=6, kind=\"bar\", palette=\"muted\")\n",
    "g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.age = pd.cut(df.age, [0, df['age'].median(), df['age'].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_per_id = df.groupby(['age', 'sex', 'pclass'])['survived'].agg(['count'])['count']\n",
    "sur_rate = df.groupby(['age', 'sex', 'pclass'])['survived'].sum().divide(total_per_id)\n",
    "sur_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sur_rate.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
