{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "sns.set_palette('hls')\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "DATA_FOLDER = join('..', '..', 'ADA2017-Tutorials', '02 - Intro to Pandas', 'Data')\n",
    "DATA_EBOLA = join(DATA_FOLDER, 'ebola')\n",
    "DATA_TITANIC = DATA_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average* per year of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Look at the data \n",
    "\n",
    "The reporting method is different in each country. Therefore we have to build different parsers. In our case, we are interested in the **daily average** of new **cases** and **death** per year. The dataset contains a lot of fields that are useless for us and that we will drop.\n",
    "Note that:\n",
    "\n",
    "* We assume that death in health workers are part of total new death/cases (e.i. total death number = patient death + health worker death).\n",
    "* We assurme that if a value is missing, it means that there is no change recorded\n",
    "\n",
    "### 1.1.1 Guinea\n",
    "Let's first read all csv files and concatenate the datas. We can directly parse the `Date` as a date entry. We fill the missing values (NaN) with 0 since it probably means that no changes were reported. The column `Total` contain the total values for each `Description` (sum of all cities). After parsing we must take a look at the duplicates to see if we have multiple entries for the same tuple (`Date`, `Description`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files and concatenate them\n",
    "guinea_path = join(DATA_EBOLA, 'guinea_data')\n",
    "guinea_files = [join(guinea_path, f) for f in listdir(guinea_path) if isfile(join(guinea_path, f))]\n",
    "\n",
    "r=[]\n",
    "for i in range(len(guinea_files)):\n",
    "    r.append(pd.read_csv(guinea_files[i], usecols=['Description', 'Totals', 'Date'], \n",
    "                         parse_dates=['Date']).fillna(0))\n",
    "    \n",
    "r = pd.concat(r)\n",
    "print('Contains duplicates:', any(r.duplicated(subset=['Date', 'Description'])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no duplicates were found we can pivot the table and keep the `Date` as index and `Totals` in `Description` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = r.pivot_table(index='Date', columns='Description', values='Totals', aggfunc='max').fillna(0)\n",
    "r.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said above, most of the columns are useless in our case. Let's display the entries to choose the columns that contains meaningfull values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fields containing the overall (total) cases/death can be dropped as we are interested in new declared cases/death. We will only keep `New cases of confirmed`, `New cases of probables`, `New cases of suspects`, `New deaths registered`, `New deaths registered today (confirmed)`, `New deaths registered today (probables)` and `New deaths registered today (suspects)` since they are more likely to contain meaningfull information for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[['New cases of confirmed', 'New cases of probables', 'New cases of suspects', 'New deaths registered', \n",
    "   'New deaths registered today (confirmed)', 'New deaths registered today (probables)', \n",
    "   'New deaths registered today (suspects)']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the fields were not properly parsed (type is object instead of int). Therefore we will apply **to_numeric** function to cast them to numbers allowing us to use basic mathematical operation.\n",
    "\n",
    "We create new fields that will be used to merge all the data (for all the countries). `n_case` contains the new cases, `n_case_un` the probable/suspected cases, `n_death` the new registred deaths and `n_death_un` the deaths suspected/probable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r['n_case'] = pd.to_numeric(r['New cases of confirmed'])\n",
    "r['n_case_un'] = pd.to_numeric(r['New cases of probables']) + pd.to_numeric(r['New cases of suspects'])\n",
    "r['n_death'] = pd.to_numeric(r['New deaths registered']) + pd.to_numeric(r['New deaths registered today (confirmed)'])\n",
    "r['n_death_un'] = pd.to_numeric(r['New deaths registered today (probables)']) + pd.to_numeric(r['New deaths registered today (suspects)'])\n",
    "r['country'] = ['guinea']*len(r['n_case'])\n",
    "guinea_res = r[['country', 'n_case', 'n_case_un', 'n_death', 'n_death_un']]\n",
    "guinea_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Liberia\n",
    "Almost the same as Guinea data. We directly parse the `Date` as a date entry. We fill the missing values with 0 since it probably means that no changes were reported. The column `National` contain the total values for each `Variable` (sum of all cities). After parsing we take a look at the duplicates to see if we have multiple entries for the same tuple (`Date`, `Variable`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files and concatenate them\n",
    "liberia_path = join(DATA_EBOLA, 'liberia_data')\n",
    "liberia_files = [join(liberia_path, f) for f in listdir(liberia_path) if isfile(join(liberia_path, f))]\n",
    "\n",
    "r_l=[]\n",
    "for i in range(len(liberia_files)): \n",
    "    r_l.append(pd.read_csv(liberia_files[i], usecols=['Date', 'Variable', 'National'], \n",
    "                         parse_dates=['Date']).fillna(0))\n",
    "    \n",
    "r_l = pd.concat(r_l)\n",
    "print('Contains duplicates:', any(r_l.duplicated(subset=['Date', 'Variable'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contain duplicates. We need to handle them. Let's take a look at the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l[r_l.duplicated(subset=['Date', 'Variable'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that those fields are not very relevant for our task. Therefore we can either drop them or merge them. We chose to merge them, using max function (for each duplicated variable we keep one with the higher value), to avoid losing data. Then we can pivot the table as we did for the Guinea data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l = r_l.pivot_table(index='Date', columns='Variable', values='National', aggfunc=max).fillna(0)\n",
    "r_l.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the column are useless in our case. Let's display the entries to choose the columns that contains meaningfull values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, all fields containing the overall (total) cases/death will be dropped. We will only keep `'New Case/s (Probable)`, `New Case/s (Suspected)`, `New case/s (confirmed)` and `Newly reported deaths)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l[['New Case/s (Probable)', 'New Case/s (Suspected)', 'New case/s (confirmed)', 'Newly reported deaths']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the same fields as for Guinea to match the data schema that we choosed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l['n_case'] = r_l['New case/s (confirmed)']\n",
    "r_l['n_case_un'] = r_l['New Case/s (Suspected)'] + r_l['New Case/s (Probable)']\n",
    "r_l['n_death'] = r_l['Newly reported deaths']\n",
    "r_l['country'] = ['liberia']*len(r_l['n_case'])\n",
    "liberia_res = r_l[['country', 'n_case', 'n_case_un', 'n_death']]\n",
    "liberia_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Sierra Leone\n",
    "Same logic as before. We directly parse the `date` as a date entry. We fill the missing values with 0 since it probably means that no changes were reported. The column `National` contain the total values for each description (sum of all cities). After parsing we take a look at the duplicates to see if we have multiple entries for the same tuple (`date`, `variable`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files and concatenate them\n",
    "sl_path = join(DATA_EBOLA, 'sl_data')\n",
    "sl_files = [join(sl_path, f) for f in listdir(sl_path) if isfile(join(sl_path, f))]\n",
    "\n",
    "r_sl=[]\n",
    "for i in range(len(sl_files)): \n",
    "    r_sl.append(pd.read_csv(sl_files[i], usecols=['date', 'variable', 'National'], \n",
    "                         parse_dates=['date']).fillna(0))\n",
    "    \n",
    "r_sl = pd.concat(r_sl)\n",
    "print('Contains duplicates:', any(r_sl.duplicated(subset=['date', 'variable'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contain duplicates, so let's look at the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl[r_sl.duplicated(subset=['date', 'variable'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as seen before, those fields are not relevant to our task. Therefore we can either drop them or merge them as we already did for the previous data. We also choose to merge them using max function to avoid data loss. Then we use the same method as before to pivot the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl = r_sl.pivot_table(index='date', columns='variable', values='National', aggfunc=max).fillna(0)\n",
    "r_sl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the column are useless in our case. Let's display the entries to choose the columns that contains meaningfull values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fields containing the overall (cumulative) of cases/death will be dropped. We will only keep `'new_confirmed`, `new_probable`, `new_suspected` and `death_confirmed` since they are more likely to contain the information we want.\n",
    "\n",
    "Note that `death_confirmed` migth contain the overall value of death. Let's take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl[['new_confirmed', 'new_probable', 'new_suspected', 'death_confirmed']].head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, `death_confirmed` contains the overall number of death. Moreover we can see that some fields are filled with 0. We assumed that it means that data were probably missing. Let's take a deeper look at those data with missing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl.loc[r_sl['death_confirmed']==0, ['new_confirmed', 'new_probable', 'new_suspected', 'death_confirmed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all fields are empty we can drop thoses entries. Afterward we can estimate the number of new death as the difference of total registred deaths between two days. Note that for day 1 (first entry in the table) we will not be able to estimate the amount of new deaths. Therefore we chose it as our starting point and set its value to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_sl.drop(r_sl.loc[r_sl['death_confirmed']==0].index, inplace=True)\n",
    "r_sl['new_death'] = pd.to_numeric(r_sl['death_confirmed']).diff().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the new fields to match the data schema that we choosed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl['n_case'] = pd.to_numeric(r_sl['new_confirmed'])\n",
    "r_sl['n_case_un'] = pd.to_numeric(r_sl['new_probable'] + r_sl['new_suspected'])\n",
    "r_sl['n_death'] = pd.to_numeric(r_sl['new_death'])\n",
    "r_sl['country'] = ['sl']*len(r_sl['n_case'])\n",
    "sl_res = r_sl[['country', 'n_case', 'n_case_un', 'n_death']]\n",
    "sl_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all data have the same structure we can concatenate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = pd.concat([guinea_res, liberia_res, sl_res]).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the evolution of number of cases and death for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax2) = plt.subplots(1, 2, figsize=(16,6))\n",
    "for label, df in r.groupby('country'):\n",
    "    df.plot(y='n_death', ax=ax, label=label)\n",
    "for label, df in r.groupby('country'):\n",
    "    df.plot(y='n_case', ax=ax2, label=label)\n",
    "ax.grid(); ax2.grid()\n",
    "ax.set_xlabel('Date'); ax2.set_xlabel('Date')\n",
    "ax.set_ylabel('# Daily deaths'); ax2.set_ylabel('# Daily cases')\n",
    "ax.set_title('Countries - Daily registred deaths'); ax2.set_title('Countries - Daily registred cases')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1. Negative number of death\n",
    "Note that for Sierra Leon (sl), there is a number of registred death negative around early october. This is, of course, not due to the fact that people resuscitated. Below is a more detailed view of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sl.iloc[40:50][['death_confirmed', 'new_death']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem appears between the 2014-09-30 and 2014-10-01 where the number of death decreased. The problem seems to be a typo (550 typed instead of 530) or a wrong estimation of number of death in a specific city. In both case we are not able to determine the real value of this field so we drop 2014-09-30 (no changes) and put 2014-10-01 to **5** (532-527) to be consistant with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = r.loc[np.logical_or(r['country']!='sl', r.index != '2014-09-30')]\n",
    "r.loc[r['n_death'] < 0, 'n_death'] = 5\n",
    "r[r['country']=='sl'].loc['2014-09-28':'2014-10-04']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2. Liberia sudden cases peak\n",
    "We can also see a huge peak at the end for the Liberia. it could be a sudden increase of registred cases but there is no correlation with the number of death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l.loc['2014-12-01':'2014-12-09'][['Total probable cases', 'New Case/s (Probable)',  \n",
    "                                    'Total suspected cases', 'New Case/s (Suspected)', \n",
    "                                    'Total confirmed cases', 'New case/s (confirmed)', 'Newly reported deaths']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it seems that there has been an error when the data was reported, like if the information had been put in a wrong column with a shift to the right starting in 2014-12-04 entry. By looking closely we see also that the numbers are oddly inferior to what has been reported before which should not be the case as the number should report a cumulative number, hence be superior. For those reasons, we decided to drop those last six entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r.drop(r.loc[np.logical_and(r.index >= '2014-12-04', r['country']=='liberia')].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.3. Final plot and results (cleared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax2) = plt.subplots(1, 2, figsize=(16,6))\n",
    "for label, df in r.groupby('country'):\n",
    "    df.plot(y='n_death', ax=ax, label=label)\n",
    "for label, df in r.groupby('country'):\n",
    "    df.plot(y='n_case', ax=ax2, label=label)\n",
    "ax.grid(); ax2.grid()\n",
    "ax.set_xlabel('Date'); ax2.set_xlabel('Date')\n",
    "ax.set_ylabel('# Daily deaths'); ax2.set_ylabel('# Daily cases')\n",
    "ax.set_title('Countries - Daily registred deaths'); ax2.set_title('Countries - Daily registred cases')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can finally compute the number of death and new cases. We add in the table the `n_case_tot` and `n_death_tot` that take into account the probable cases/deaths. Each column is expressed as the **daily average** case/death **per year**\n",
    "\n",
    "We can see on the next table that the average number of death per day (`n_death`) is more important in Liberia and Sierra Leone. If we consider the probable cases `n_case_tot`, there are more cases of ebola in Sierra Leone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r['n_case_tot'] = r['n_case'] +  r['n_case_un']\n",
    "r['n_death_tot'] = r['n_death'] +  r['n_death_un']\n",
    "\n",
    "COUNTRIES = ['guinea', 'liberia', 'sl']\n",
    "ds = [(r[r['country']==COUNTRY].index[-1]-r[r['country']==COUNTRY].index[0]).days for COUNTRY in COUNTRIES]\n",
    "print('Days spans: {}'.format(ds))\n",
    "r.groupby('country').sum().divide(ds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "microbiome_path = join(DATA_FOLDER, 'microbiome')\n",
    "\n",
    "# Read 9 first speadsheets and concatenate them\n",
    "microbiome_files = [join(microbiome_path, f) for f in listdir(microbiome_path) if isfile(join(microbiome_path, f))]\n",
    "microbiome_files.sort()\n",
    "\n",
    "#Save and remove the metadata path\n",
    "metadata = microbiome_files[-1]\n",
    "microbiome_files.pop();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[]\n",
    "for i in range(len(microbiome_files)):\n",
    "    df_temp = pd.read_excel(microbiome_files[i], header=None, names=['Name', 'Number'])\n",
    "    df_temp['src'] = i\n",
    "    arr.append(df_temp)\n",
    "\n",
    "arr = pd.concat(arr)\n",
    "arr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_header = pd.read_excel(metadata)\n",
    "col_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of duplicates: {}'.format(np.sum(arr.duplicated(subset=['Name', 'src']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_merged = arr.pivot(index='Name', columns='src', values='Number')\n",
    "arr_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = col_header[['GROUP', 'SAMPLE']].fillna('unknown').values.T\n",
    "index_new = pd.MultiIndex.from_tuples(list(zip(*t)), names=['GROUP', 'SAMPLE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_merged.columns = index_new\n",
    "arr_merged.fillna('unknown', inplace=True)\n",
    "arr_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Is index unique: {}'.format(arr_merged.index.is_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(join(DATA_TITANIC, 'titanic.xls'),  \n",
    "                   converters={'pclass': np.int, 'survived': np.int, 'age': np.float})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first glance, we can use the html file to have the type and range for some attributes. Some attributes will need further investigations. Let's have a look at how the data is organized column by column : \n",
    "* `pclass`: Numerical value of type int64 at first, this field can take 1,2,3 as value. As it has only 3 fixed value, it can be categorical. <br>\n",
    "We check it contains indeed only 3 values (1,2,3). Then we set it as categorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['pclass'].dtype))\n",
    "print('Unique categories: {}'.format(df['pclass'].unique()))\n",
    "df['pclass'] = df.pclass.astype('category', ordered=True)\n",
    "df['pclass'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `survived`: Binary class, hence numerical value that will be 0 or 1. Type in64.It could be set as categorical. Actually we decided to keep it as numerical to simplify the way to plot survival ratio. <br>\n",
    "We check it contains indeed only 2 values (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['survived'].dtype))\n",
    "print('Unique categories: {}'.format(df['survived'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `name`: Passenger name. Simple string value but stored as an object type. Not categorical. Many possiblities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['name'].dtype))\n",
    "print('Unique categories: {}'.format(df['name'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `sex`: String corresponding to sex of the person with two possibility : female or male. Stored as an object type Can be categorical. <br>\n",
    "We check if it contains indeed only two possibility before setting it as a categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['sex'].dtype))\n",
    "print('Unique categories: {}'.format(df['sex'].unique()))\n",
    "df['sex'] = df.sex.astype('category', ordered=True)\n",
    "df['sex'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `age`: Numerical float64 indicating age of the passenger, It should be a positive number and not too high or NaN. We have 263 personne with no registered age and an age value in range [0.1667, 80]. There are a lot of passengers that do not have age entries. We will let them set as NaN and discard them for plotting. Not categorical.\n",
    "\n",
    "Note that we were sceptical when we saw that the minimum age was 0.1667 since it is not an integer. But as you can see (in the next paragraph) all the values between 0 and 1 are actually the age of the baby in month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['age'].dtype))\n",
    "print('Amount of NaN: {}, max: {}, min: {}'.format(\n",
    "    np.sum(pd.isnull(df['age'])), df['age'].max(), df['age'].min()))\n",
    "print('Baby age in range 0-1 : {} month/s'.format( \n",
    "    np.round(pd.to_numeric(df[df['age'] < 1]['age'].values*12), decimals=1) ))\n",
    "df.age = pd.to_numeric(df.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `sibsp`: Familly related field (number of siblings and/or spouse on boat). Stored as int64 type and range in [0,8] Not categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['sibsp'].dtype))\n",
    "print('Amount of NaN: {}, max: {}, min: {}'.format(\n",
    "    np.sum(pd.isnull(df['sibsp'])), df['sibsp'].max(), df['sibsp'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `parch`: Family related field (number of parents and/or children on boat). Stored as int64 and range in [0,9]. Not categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['parch'].dtype))\n",
    "print('Amount of NaN: {}, max: {}, min: {}'.format(\n",
    "    np.sum(pd.isnull(df['parch'])), df['parch'].max(), df['parch'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `ticket`: String value indicating id of the ticket. Contain letters and numbers and has many possibilities, stored as an object type. Not categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['ticket'].dtype))\n",
    "print('Ticket id possibilities: {}'.format(df['ticket'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `fare`: Numerical value, float64, indicating ticket fare, expressed in British pound. Range 0 to 512.3292. Not categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['fare'].dtype))\n",
    "print('Amount of NaN: {}, max: {}, min: {}'.format(\n",
    "    np.sum(pd.isnull(df['fare'])), df['fare'].max(), df['fare'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `cabin`: String that contains the cabin number and the floor, stored as an object. Take many possiblities\n",
    "    * `floor`: Letter indicate the floor number. We will set it as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['cabin'].dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can split the field and isolate the floor letter. To do so we use regex. Since some fields contains multiple letters we only keep the first one as the are the same. ex: `B58 B60` -> `BB` -> `B`. Note that `n` is not a floor but the abreviation of `nan`, so thoses are missing values. We set them to NaN and we don't drop them since there is only 2 values. We will only not take them into account when plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floors = [re.sub(r'[0-9 ]', '', str(item))[0] for item in df['cabin']]\n",
    "print('Floors (unique): {}'.format(np.unique(floors)))\n",
    "df['floor'] = floors\n",
    "df.loc[df['floor'] == 'n', 'floor'] = np.NaN\n",
    "df['floor'] = df.floor.astype('category', ordered=True)\n",
    "df['floor'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `embarked`: String stored as an object type, indicating embarcation location with 3 possible values : Cherbourg (C), Queenstown (Q), Southampton (S). Categorical <br>\n",
    "We check if it contains only three values Cherbourg (C), Queenstown (Q), Southampton (S). However we can see that it contains also unknown values (nan). As you see there are only 2 values that containt NaN. Therefore we will let them set to NaN and not include them in `embarked` plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['embarked'].dtype))\n",
    "print('Unique categories: {}, Unknown: {}'.format(df['embarked'].unique(), df['embarked'].isnull().sum()))\n",
    "df['embarked'] = df['embarked']\n",
    "df['embarked'] = df.embarked.astype('category', ordered=True)\n",
    "df['embarked'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `boat`: String stored as an object type that indicates the rescue boat id (not sure about this one). Many possible outcomes, not categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['boat'].dtype))\n",
    "print('Boat id possibilities: \\n{}'.format(df['boat'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `body`: Numerical value that indicates the body id number, type float64 in range [1,328]. Many NaN. Not categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['body'].dtype))\n",
    "print('Amount of NaN: {}, max: {}, min: {}'.format(\n",
    "    np.sum(pd.isnull(df['body'])), df['body'].max(), df['body'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `home.dest`: String indicating the home and final destination of the passenger, stored as a object type. Many possible outcomes. Not categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type of data: {}'.format(df['home.dest'].dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : If ranges was no specified it means that the values can either be a string (no limit) either be a mix between numbers and strings (`cabin` or `boat` for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Histogram data\n",
    "We plot the repartition of the passenger as a function of `pclass`, `sex`, `embarked` and `age`. Note that, as said before, NaN values are not taken into account hence are not part of the results since they do not represent usefull data.\n",
    "\n",
    "Note that we split the `age` data into subsets (age ranges of 10 years) as asked for histogram plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nice_bar_plot(data, ax, title='', y_axis=''):\n",
    "    ax.set_title(title , fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(data.name); ax.set_ylabel(y_axis)\n",
    "    sns.barplot(x=data.value_counts().keys(), y=data.value_counts().values,  ax=ax)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.setp(labels, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16,12))\n",
    "\n",
    "nice_bar_plot(df['pclass'], axes[0, 0], 'Repartition class', '#persons')\n",
    "nice_bar_plot(df['sex'], axes[0, 1], 'Repartition sex', '#persons')\n",
    "nice_bar_plot(df['embarked'], axes[1, 0], 'Repartition embarc. location', '#persons')\n",
    "age_cut = pd.cut(df.age, [0, 10, 20, 30, 40, 50, 60, 70, df['age'].max() ])\n",
    "nice_bar_plot(age_cut, axes[1, 1], 'Repartition age', '#persons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Proportion of passengers by cabin floor\n",
    "\n",
    "We calculate and plot in a pie chart the proportion of passengers by cabin floor. We use the variable `floor` that we computed in part 1 to do so. \n",
    "\n",
    "It is important to take into account the fact that there is a lot of NaN entry and we only have the value for 295 passengers, which is not so representative from the total number of passengers (1309) in the dataset. Hence, from the observable datas, we can say that approximately 1/3 of the passengers where on C floor. We found it odd to have people located on T floor, as it seems to be the floor where the motors and turbine where located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_proportion(data):\n",
    "    proportion = (data.value_counts().divide(data.value_counts().sum())*100)\n",
    "    return proportion\n",
    "\n",
    "print('Total of passengers that have a valid entry of floor : {}'.format(df['floor'].value_counts().sum() ))\n",
    "print('Repartition by floor : \\n{}'.format(calc_proportion(df['floor'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_palette('hls', 10)\n",
    "def nice_pie_plot(data, ax, title=''):\n",
    "    ax.set_title(title , fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(data.name)\n",
    "    ax.pie(data.value_counts().values, labels=data.value_counts().keys(), autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "nice_pie_plot(df['floor'], ax, 'Repartition of passenger according to floor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Proportion of survivors by travel class\n",
    "\n",
    "Below we compute the proportion and plot it in pie charts where 1 means that the person survived and 0 means that the person died.\n",
    "Here we can say that the results are relevant as there is an entry for every of the 1309 passengers listed in our dataset. We observe that there is a much higher chance of survival in first class, 61,92% whereas it drops to 42.96% in second class and is even lower in third class at 25.52%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (label, df_sub) in enumerate(df.groupby('pclass')):\n",
    "    print(\"Proportion of survivors in class\", i+1, \"(in pourcentage) :\\n\",df_sub['survived'].value_counts(sort=False).divide(df_sub['survived'].value_counts(sort=False).sum())*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('hls', 2)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16,5))\n",
    "for i, (label, df_sub) in enumerate(df.groupby('pclass')):\n",
    "    df_sub['survived'].value_counts(sort=False).plot.pie(\n",
    "        legend=True, ax=axes[i], title='Survived ratio Class{}'.format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Proportion of passengers that survived by travel class and sex\n",
    "\n",
    "Below <font color=red>we compute the numbers **TODO** </font>  and plot an histogram that shows the survival proportion. \n",
    "The number are as relevant as before. We can see that the survival chance of a woman is much higher than the one of a man, for any class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.factorplot(x=\"pclass\", y=\"survived\", hue=\"sex\", data=df, size=6, kind=\"bar\", palette=\"muted\")\n",
    "g.set_ylabels(\"survival proportion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Several survival proportions \n",
    "\n",
    "We splitted the population by age using the median, in order to have two equally populated age categories. After we made the computation to observe the survival proportion by age category, travel class and gender with the result displayed in a Dataframe below.\n",
    "\n",
    "This is a much finer way to observe the survival rate than the one used right above. We can see that as stated before, women had a higher chance to survive. We can see also that overall, younger people had better chance to survive. AS observed before, the higher class a person was in, the higher its chances were to survive. Those observations allows us to confirm that the \"Women and children first\" rule was applied on the Titanic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.age = pd.cut(df.age, [0, df['age'].median(), df['age'].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_per_id = df.groupby(['age', 'sex', 'pclass'])['survived'].agg(['count'])['count']\n",
    "sur_rate = df.groupby(['age', 'sex', 'pclass'])['survived'].sum().divide(total_per_id)*100\n",
    "sur_rate = pd.DataFrame(sur_rate)\n",
    "sur_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sur_rate.index.is_unique"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
