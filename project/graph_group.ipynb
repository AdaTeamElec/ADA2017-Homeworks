{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Use this <a href=\"http://nbviewer.jupyter.org/github/AdaTeamElec/ADA2017-Homeworks/blob/master/project/IntroAda.ipynb\">link</a> to properly display maps </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "## Abstract\n",
    "Terrorism is a subject largely covered in the media, and, unfortunately, we became accustomed to its presence worldwide, particularly over the last decade. Nevertheless, the problem we are facing today is not new. The source of certain conflicts dates from multiple decades, some of which are still lasting today. Our goal is to track and vizualize terrorism evolution through the past 50 years based on \"The Global Terrorism Database\". There are many questions we can ask ourselves about terrorism, such as \"Is EU less safe nowadays ?\", \"Did attack mediums & reasons change over the years ?\" or \"Can we discriminate current/future conflictual zones ?\". It would be presumptuous from us to say that we are going to solve major issues, or even predict futur attacks. However, through the exploration of the dataset, and by trying to answer those interrogations, we aim to grasp an overview and a better understanding to the evolution of terrorism.\n",
    "\n",
    "## Plan\n",
    "\n",
    "1. [Raw data understanding and cleaning](#raw_data)\n",
    "    1. [Field selections using documentation](#fields_select)\n",
    "    2. [Data exploration ](#data_exploration)\n",
    "2. [Groups](#groups)\n",
    "3. [Groups territories](#groups_terr)\n",
    "\n",
    "---\n",
    "\n",
    "# 1 Raw data understanding  <a id='raw_data'></a>\n",
    "\n",
    "## 1.1 Field selections using documentation  <a id='fields_select'></a>\n",
    "\n",
    "First of all, we need to take a deep look into the details of our dataset to sort out the relevant data we will be using to conduct our observations. The Global Terrorism Dataset contains 135 features and approximately 170'000 entries. In order to select the label we will keep, we used the official [documentation](http://start.umd.edu/gtd/downloads/Codebook.pdf) from the dataset which describes each features precisely. Let's make a quick summary of the labels from the dataset we decided to use for our project.\n",
    "\n",
    "* `eventid` : this is the id of any entry, written as 12 numbers (first 8 digits are the date of event and last 4 digits are a sequential case number for the given day). This will be used as our index too.\n",
    "* `iyear`, `imonth`, `iday` : Year, month and day of the event. In some rare occasion the month or days are unknown.\n",
    "* `country_txt` : id and name of the country where the event took place.\n",
    "* `region_txt` : id and region where the event took place.\n",
    "* `city` : This field contains the name of the city, village, or town in which the incident occurred. If the city, village, or town for an incident is unknown, then this field contains the smallest administrative area below provstate which can be found for the incident (e.g., district).  \n",
    "* `latitude` and `longitude` : Latitude and Longitude values where the event took place.\n",
    "* `doubtterr` : boolean value set as 1 if there is a doubt to whether the incident is an act of terrorism and 0 if there is no doubt of a terrorist attack.\n",
    "* `success` : boolean value set as 1 if the incident was successful or 0 if it was not. As stated in the documentation, \"Success of a terrorist strike is defined according to the tangible effects of the attack. Success is not judged in terms of the larger goals of the perpetrators. For example, a bomb that exploded in a building would be counted as a success even if it did not succeed in bringing the building down or inducing government repression.\" \n",
    "* `suicide` : boolean value set as 1 if the attack perpetrator did not intend to escape from the attack alive, 0 otherwise.\n",
    "* `attacktype1_txt` : This field captures the general method of attack and often reflects the broad class of tactics used. It consists of nine categories, which are defined below :\n",
    "    1. Assassination\n",
    "    2. Armed Assault\n",
    "    3. Bombing/Explosion\n",
    "    4. Hijacking \n",
    "    5. Hostage taking (barricade incident) \n",
    "    6. Hostage taking (kidnapping)\n",
    "    7. Facility/Infrastructure Attack\n",
    "    8. Unarmed Assault\n",
    "    9. Unknown \n",
    "* `targtype1_txt` : The target/victim type field captures the general type of target/victim. When a victim is attacked specifically because of his or her relationship to a particular person, such as a prominent figure, the target type reflects that motive. For example, if a family member of a government official is attacked because of his or her relationship to that individual, the type of target is “government.” This variable consists of the following 22 categories: <br>\n",
    "    1. Business\n",
    "    2. Government (General)\n",
    "    3. Police\n",
    "    4. Military\n",
    "    5. Abortion related\n",
    "    6. Airport & aircraft\n",
    "    7. Government (Diplomatic), differs from the other entry as here are taken into account representation of a gouvernment on a foreign soil (embassy, consulate...)\n",
    "    8. Educational institution\n",
    "    9. Food or water supply\n",
    "    10. Journalist & media\n",
    "    11. Maritime facilities, including ports\n",
    "    12. NGO\n",
    "    13. Other\n",
    "    14. Private citizens & property, include attacks in a public area against private citizens\n",
    "    15. Religious figures/insititutions\n",
    "    16. Telecommunication\n",
    "    17. Terrorists/non-state militias\n",
    "    18. Tourists\n",
    "    19. Transportation (other than aviation)\n",
    "    20. Unknown\n",
    "    21. Utilities, facilities for generation or transmission of energy\n",
    "    22. Violent political parties\n",
    "* `gname` : This field contains the name of the group that carried out the attack. In order to ensure consistency in the usage of group names for the database, the GTD database uses a standardized list of group names that have been established by project staff to serve as a reference for all subsequent entries.  \n",
    "* `gname2` : This field is used to record the name of the second perpetrator when responsibility for the attack is attributed to more than one perpetrator. Conventions follow “Perpetrator Group” field.  \n",
    "* `gname3` : same as for gname2\n",
    "* `nperps` : This field indicates the total number of terrorists participating in the incident. (In the instance of multiple perpetrator groups participating in one case, the total number of perpetrators, across groups, is recorded). There are often discrepancies in information on this value.   \n",
    "* `weaptype1_txt` : This field records the general type of weapon used in the incident. It consists of the following categories: <br>\n",
    "    1. Biological\n",
    "    2. Chemical\n",
    "    3. Radiological\n",
    "    4. Nuclear\n",
    "    5. Firearms\n",
    "    6. Explosive/bonbs/dynamite\n",
    "    7. Fake weapons\n",
    "    8. Incendiary\n",
    "    9. Melee\n",
    "    10. Vehicle\n",
    "    11. Sabotage equipment \n",
    "    12. Other\n",
    "    13. Unknown\n",
    "* `nkill` : This field stores the number of total confirmed fatalities for the incident. The number includes all victims and attackers who died as a direct result of the incident.   \n",
    "* `nkillter`: This field stores the number of confirmed terrorists fatalities.\n",
    "* `nwound` : This field records the number of confirmed non-fatal injuries to both perpetrators and victims. \n",
    "* `nwoundte` : This field records the number of confirmed non-fatal terrorists injuries. \n",
    "\n",
    "\n",
    "We are now reduced to 22 features instead of the original 135 from the dataset. A part from the kept features, we explored some other features such as `weaptype2`, `weapsubtype` or `motive` to see if those would bring added informations thus be relevant to use also. However, we decided to drop them because of a too large amount of NaN or unknown entries. Our choice focused on labels that would allow us to answer the questions asked in the description, as well as labels relevant to get a pertinent visualization of the data.\n",
    "\n",
    "\n",
    "## 1.2 Data exploration  <a id='data_exploration'></a>\n",
    "\n",
    "Let's begin the work by importing the libraries and creating a dataframe to explore the data furthermore. As cautious wannabe data scientist, we will explore in detail each field and check the proportion of non categorized or Unknown-labeled entries to make sure each feature we kept countains relevant data. <br>\n",
    "*NOTE : during this section we are only exploring data without drawing any conclusions nor making assumptions regarding the data. This will come further in our analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "import warnings\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "gtd_path = os.path.join(data_path, 'globalterrorismdb_0617dist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = ['eventid', 'iyear', 'imonth', 'iday', 'country_txt', 'region_txt', 'city', \n",
    "          'latitude', 'longitude', 'doubtterr', 'attacktype1_txt',  'success', \n",
    "          'suicide', 'weaptype1_txt', 'targtype1_txt', 'gname', 'gname2', \n",
    "          'gname3', 'compclaim', 'nperps', 'nkill', 'nkillter', 'nwound', 'nwoundte']\n",
    "date_fileds = ['iyear', 'imonth', 'iday']\n",
    "\n",
    "df = pd.read_csv(gtd_path, encoding='latin', usecols=fields, index_col='eventid', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Is index unique: {}'.format(df.index.is_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, month or day (or both) can be set to 0 if the exact date of the attack is unknown. We created a function to set the value to of the field to 1 in the case of an unknown date. We then count the proportion of unknown date within the dataset, just to make sure it is not too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# According to documentation both month and day can be 0 (if unknown), we set them to 0\n",
    "def parse_date(row):\n",
    "    return datetime.date(row.iyear, int(row.imonth) if not np.isnan(row.imonth) else 1, \n",
    "                         int(row.iday) if not np.isnan(row.iday) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count number entries with uncertain date (either month or day)\n",
    "df[date_fileds] = df[date_fileds].replace(0, np.nan)\n",
    "n_uncertain = np.sum(np.sum(df[date_fileds].isnull(), axis=1) != 0 )\n",
    "df['date'] =df.apply(lambda x: parse_date(x), axis=1)\n",
    "print('Uncertain dates: {:.2f}%, ({}/{})'.format(100*n_uncertain/len(df), n_uncertain, len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the proportion of entries without geographic coordinates. In the case when the coordinates are unknow, we decided to completely drop the row of data. This is due to the fact that we want to have the location informations in order to represent the data with maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_geo = len(df)\n",
    "df.dropna(subset=('latitude', 'longitude'), inplace=True)\n",
    "print('Entries without geographic coordinates droped: {:.2f}%, ({}/{})'.format(\n",
    "    100*(n_geo-len(df))/n_geo, n_geo-len(df), n_geo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that each item has a valid entry for city, country or region. We see that it is missing a few entries for city name, but as we will not use this feature often, we won't drop those entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Proportion of data without a valid city entry: {:.2f}%'.format(100*(1-df.city.value_counts().sum()/len(df))))\n",
    "print('Proportion of data without a valid country entry: {:.2f}%'.format(100*(1-df.country_txt.value_counts().sum()/len(df))))\n",
    "print('Proportion of data without a valid region entry: {:.2f}%'.format(100*(1-df.region_txt.value_counts().sum()/len(df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now checking the amount of attacks that are categorized as unsure terror attacks. We wanted to check this particular feature to make sure that in the dataset there is a large majority of attacks that are hundred percent sure to be terror attack. If it would not have been the case, the whole dataset as well as our study would not have been relevant. <br>\n",
    "There is 15% of attacks for which there is a doubt to categorize them as terror attacks. The entry -9 represent cases for which the value was not available at all when the dataset was constructed. We decided to assign them as if it was sure they were terror attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Repartition of data in %:\\n{}'.format(100*df.doubtterr.value_counts()/len(df)))\n",
    "df.loc[df.doubtterr < 0, 'doubtterr'] = 0\n",
    "df.doubtterr = df.doubtterr.astype('category')\n",
    "df.doubtterr.cat.categories = ['N_DOUBT', 'DOUBT']\n",
    "print('\\nRepartition of data in %(after cleaning):\\n{}'.format(100*df.doubtterr.value_counts()/len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the two upcoming fields, data is, as expected, binary and completly categorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Unique values in field: {}'.format(np.unique(df.success)))\n",
    "print('Percentage of sucessful attacks: {:.2f}%'.format(100*df.success.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Unique values in field: {}'.format(np.unique(df.suicide)))\n",
    "print('Percentage of suicide attacks: {:.2f}%'.format(100*df.suicide.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to explore if the proportion of attack types and see if there are any NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Type of attack and repartition in dataset in %')\n",
    "100*df.attacktype1_txt.value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the repartion of target types and the repartion of weapon types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Type of target and repartition in dataset in %')\n",
    "100*df.targtype1_txt.value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Repartition of weapon type in dataset in %')\n",
    "100*df.weaptype1_txt.value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check the number of unique entries that categorize the name of the group conducting the terror attacks. According to the documentation, a work as been done to standardize the entries within this field by using a specific list of group names established by project staff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.value_counts(df[['gname', 'gname2', 'gname3']].values.ravel('K')).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['gname', 'gname2', 'gname3']] = df[['gname', 'gname2', 'gname3']].replace({'Unknown': np.nan})\n",
    "n_group = len(pd.unique(df[['gname', 'gname2', 'gname3']].values.ravel('K')))\n",
    "print('Number of unique group name: {}'.format(n_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore data with numerical values. First we look at field corresponding to the number of perpretrators of an attack. As expected, it countains a large amount of unknown entries as it is not easy to know how many perpetrators of an attack there was. We decided to keep this row anyway as to explore, if possible, the evolution of terror attack, and the number of perpetrators is a value that could give an insight to know this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc[df.nperps < 0, 'nperps'] = np.nan\n",
    "print('Percentage of entries with unknown # Perpretrators {:.2f}%'.format(100*np.sum(df.nperps.isnull())/len(df)))\n",
    "print('Range of # Perpretrators: {} upto {}'.format(int(df.nperps.min()), int(df.nperps.max())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look now at the number of victims of terror attacks. As the dataset count the total number of fatalities, perpetrators included, we found it relevant to keep also the number of killed terrorists to conduct our analysis. Same logic applies for the number of wounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['nkillnter'] = df.nkill-df.nkillter.fillna(0)\n",
    "df.loc[df.nkillnter < 0, 'nkillnter'] = 0\n",
    "\n",
    "print('Range total # of victims: [{}, {}]'.format(df.nkill.min(), df.nkill.max()))\n",
    "print('Range # of non terrorists victims: [{}, {}]'.format(df.nkillnter.min(), df.nkillnter.max()))\n",
    "print('Range # of terrorists victims: [{}, {}]'.format(df.nkillter.min(), df.nkillter.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['nwoundnter'] = df.nwound-df.nwoundte.fillna(0)\n",
    "df.loc[df.nwoundnter < 0, 'nwoundnter'] = 0\n",
    "\n",
    "print('Range total # of wounded: [{}, {}]'.format(df.nwound.min(), df.nwound.max()))\n",
    "print('Range # of non terrorists wounded: [{}, {}]'.format(df.nwoundnter.min(), df.nwoundnter.max()))\n",
    "print('Range # of terrorists wounded: [{}, {}]'.format(df.nwoundte.min(), df.nwoundte.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a better understanding of our dataset and we are confident to have a dataset we can work with. We can go further in our analysis and do data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Groups  <a id='groups'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have for each group: the number of attacks (`frequ`), number of casualities (`nkill`), coordinates (`latitude`, `longitude`) and country (`country`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_groups = pd.read_csv(os.path.join(data_path, 'groups_stats.csv'), index_col=0)\n",
    "df_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_min = 20\n",
    "len(df_groups.loc[df_groups.frequ > N_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 2d table with for matching pair-wise groups\n",
    "frames = [ df[['gname', 'gname2', 'compclaim']].values, \n",
    "           df[['gname', 'gname3', 'compclaim']].values, \n",
    "           df[['gname2', 'gname3', 'compclaim']].values ]\n",
    "df_multi = pd.DataFrame(np.array(frames).reshape((-1, 3)))\n",
    "df_multi.columns = ['gname', 'gname2', 'compclaim']\n",
    "df_multi.compclaim = df_multi.compclaim.replace({-9: np.nan})\n",
    "# Drop line with NaN (not relevant matching)\n",
    "df_multi.dropna(subset=('gname', 'gname2', 'compclaim'), inplace=True)\n",
    "print('Number of colaboration/comp:', len(df_multi))\n",
    "df_multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set attrbute value to 1\n",
    "df_coop = df_multi.loc[df_multi.compclaim == 0].groupby(['gname', 'gname2']).size().reset_index(name='n_coop')\n",
    "df_comp = df_multi.loc[df_multi.compclaim == 1].groupby(['gname', 'gname2']).size().reset_index(name='n_comp')\n",
    "df_link = pd.merge(df_coop, df_comp, how='outer').fillna(0)\n",
    "df_link.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1 Signed Graph clustering example\n",
    "\n",
    "Based on [Spectral Analysis of Signed Graphs for Clustering, Prediction and Visualization](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.155.1809&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils.signedgraph as sg\n",
    "\n",
    "W = sg.get_dummy()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "ax = plt.subplot(1,1,1)\n",
    "sg.draw_graph(W, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = sg.compute_L(W, normalized=False)\n",
    "cgt = sg.get_clusters(L, 3)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "ax = plt.subplot(1,1,1)\n",
    "sg.draw_graph(W, cgt, ax=ax)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def plot_clusters(cluster):\n",
    "    # Space weight \n",
    "    n_space = 2*cluster['ncluster'] + len(cluster['est_cgt'])\n",
    "    _, counts = np.unique(cluster['est_cgt'], return_counts=True)\n",
    "    intervals = 2*pi*np.cumsum(counts)/(len(cluster['est_cgt']) + 2*cluster['ncluster'])\n",
    "    v = 4*pi/(len(cluster['est_cgt']) + 2*cluster['ncluster'])\n",
    "    plt.figure(figsize=(12,12))\n",
    "    ax = plt.subplot(1, 1, 1, polar=True)\n",
    "    cmap = cm.get_cmap(name='Set3')\n",
    "    \n",
    "    for i, interval in enumerate(intervals):\n",
    "        if i == 0:\n",
    "            start = 0 + i*v\n",
    "        else:\n",
    "            start = intervals[i-1] + i*v\n",
    "        end = intervals[i] + i*v\n",
    "        \n",
    "        xval = np.linspace(start, end, np.ceil(100*(end-start)))\n",
    "        yval = np.ones_like(xval)\n",
    "        ax.scatter(xval, yval, c=cmap(i), s=300, linewidths=0)\n",
    "        \n",
    "    print(np.argsort(cluster['est_cgt']))\n",
    "    #colormap = plt.get_cmap('Set3')\n",
    "    #norm = mpl.colors.Normalize(0.0, 2*np.pi)\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax.text(0.5, 0.5, 'text 0', rotation=0)\n",
    "\n",
    "plot_clusters(groups_clustering[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 Graph clustering -> data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "import networkx as nx\n",
    "\n",
    "groups = pd.unique(df_link[['gname', 'gname2']].values.ravel('K'))\n",
    "d = dict(zip(groups, np.arange(len(groups))))\n",
    "\n",
    "# Create Adjacency matrix with # of attack claimed together as weight between links (e.i. groups)\n",
    "W_coop = csc_matrix((df_link['n_coop'], \n",
    "                (df_link['gname'].replace(d), df_link['gname2'].replace(d))), shape=(len(d), len(d)))\n",
    "W_coop = 0.5*(W_coop.T+W_coop)\n",
    "\n",
    "W_comp = csc_matrix((df_link['n_comp'], \n",
    "                (df_link['gname'].replace(d), df_link['gname2'].replace(d))), shape=(len(d), len(d)))\n",
    "W_comp = 0.5*(W_comp.T+W_comp)\n",
    "\n",
    "W = (W_coop - W_comp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.from_scipy_sparse_matrix(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groups_clustering = {}\n",
    "clusters = [list(g) for g in nx.connected_components(G) if len(g) >= 8]\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print('Cluster length: {}\\nNames: {}\\n'.format(len(cluster), groups[cluster]))\n",
    "    # Extract adjacency matrix fpr specific cluster\n",
    "    W = sg.get_w_signed(W_coop, W_comp, cluster, binary=False)\n",
    "    # Compute Nomralized signed laplacian\n",
    "    L = sg.compute_L(W, normalized=True)\n",
    "    # Get estimate of number of clusters\n",
    "    ncluster = sg.estimate_ncluster(W, L, plotloss=False)\n",
    "    \n",
    "    # Get estimated clustering\n",
    "    est_cgt = sg.get_clusters(L, ncluster)\n",
    "    # Save results as dictionary\n",
    "    groups_clustering[i] = {'W': W, 'ids': cluster, 'ncluster': ncluster, 'est_cgt': est_cgt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12*len(groups_clustering)))\n",
    "\n",
    "for i, item in enumerate(groups_clustering):\n",
    "    ax = plt.subplot(len(groups_clustering), 1, i+1)\n",
    "    ax.set_title('Clustered (N={}) and organized graph'.format(groups_clustering[item]['ncluster']), fontsize=14)\n",
    "    sg.draw_graph(groups_clustering[item]['W'], \n",
    "                  groups_clustering[item]['est_cgt'], \n",
    "                  labels=groups[groups_clustering[item]['ids']], ax=ax, reorder=True, offset=4e-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# 3. Groups  Territories <a id='groups_terr'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import utils.heat_gsp as heat_gsp\n",
    "from pygsp import graphs, filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Graph and signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take only Middle East & North Africa points\n",
    "df_ME = df.loc[df.region_txt == 'Middle East & North Africa']\n",
    "df_ME['id_loc'] = df_ME.latitude.astype(str) + '_' + df_ME.longitude.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop duplicates to build map\n",
    "nodes = df_ME.drop_duplicates(subset='id_loc')\n",
    "nodes = nodes.reset_index().reset_index()\n",
    "nodes = nodes[['latitude', 'longitude', 'id_loc', 'index']].set_index('id_loc')\n",
    "W = heat_gsp.knn_graph(nodes)\n",
    "print('Are nodes unique: {}'.format(nodes.index.is_unique))\n",
    "print('Adjacency matrix shape: {}'.format(W.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "frequ_min = 10\n",
    "cls_groups = pd.value_counts(df_ME[['gname', 'gname2', 'gname3']].values.flatten())\n",
    "cls_groups = cls_groups[cls_groups.values > frequ_min].index.values\n",
    "cls_groups = np.random.permutation(cls_groups)\n",
    "print('Number of groups with at least {} attacks: {}'.format(frequ_min, len(cls_groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_time_interval = 10\n",
    "intervals = np.linspace(0, len(df_ME)-1, n_time_interval+1).astype(int)\n",
    "dates = df_ME.iloc[intervals].date.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_time_inveral = df_ME.loc[np.logical_and(df_ME.date >= dates[9], df_ME.date <= dates[10])]\n",
    "signal = heat_gsp.get_signal_attack(df_time_inveral, nodes, cls_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = graphs.Graph(W)\n",
    "G.estimate_lmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_ISIS = np.where(cls_groups == 'Islamic State of Iraq and the Levant (ISIL)')[0][0]\n",
    "heat_gsp.plot_map_group(nodes, signal[:, id_ISIS], title='Attacks prformed by {}'.format(cls_groups[id_ISIS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g_heat = filters.Heat(G, tau=100)\n",
    "signal_heat = g_heat.filter(signal, method='chebyshev')\n",
    "heat_gsp.plot_map_group(nodes, signal_heat[:,id_ISIS], \n",
    "                        title='Attacks prformed by {} - Spread'.format(cls_groups[id_ISIS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signal_heat_thresh = np.concatenate((5e-2*np.ones((signal_heat.shape[0], 1)), signal_heat), axis=1)\n",
    "id_groups = np.argmax(signal_heat_thresh, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heat_gsp.plot_map_cls(nodes, id_groups, cls_groups, 'Groups repartition in late 2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
